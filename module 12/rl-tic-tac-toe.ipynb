{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIT320 Advanced Algorithms\n",
    "## Module 12 - MDP and Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"uml-class-diagram.png\" width=\"1000\" height=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Board Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Board(ABC):\n",
    "    def __init__(self, boardDimensions):\n",
    "        self.boardDimensions = boardDimensions\n",
    "\n",
    "    @abstractmethod\n",
    "    def createBoard(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def printBoard(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def setCellState(self, position, state):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def getCellState(self, position):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def getBoardState(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def getBoardDimensions(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def spaceIsFree(self, position):\n",
    "        pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "class TicTacToeBoard(Board):\n",
    "    def __init__(self, boardDimensions):\n",
    "        \"\"\"Create a board of dimensions boardDimensions x boardDimensions\n",
    "        Args: boardDimensions (int): the dimensions of the board\n",
    "        \"\"\"\n",
    "        self.boardDimensions = boardDimensions\n",
    "        self.createBoard()\n",
    "\n",
    "\n",
    "    def createBoard(self):\n",
    "        \"\"\"Create a board of dimensions boardDimensions x boardDimensions\"\"\"\n",
    "        self.boardState = {i+1: ' ' for i in range(self.getBoardDimensions()**2)}\n",
    "\n",
    "    def printBoard(self):\n",
    "        for i in range(self.boardDimensions):\n",
    "            row = [self.boardState[i*self.boardDimensions+j+1] for j in range(self.boardDimensions)]\n",
    "            print('|'.join(row))\n",
    "            if i < self.boardDimensions-1:\n",
    "                print('-'*(self.boardDimensions*2-1))\n",
    "        print('\\n')\n",
    "\n",
    "    def setCellState(self, position, state):\n",
    "        \"\"\"Set the state of a cell on the board\n",
    "        Args:\n",
    "            position (int): the position of the cell\n",
    "            state (str): the state of the cell\n",
    "        \"\"\"\n",
    "        self.boardState[position] = state\n",
    "\n",
    "    def getCellState(self, position):\n",
    "        \"\"\"Get the state of a cell on the board\n",
    "        Args:\n",
    "            position (int): the position of the cell\n",
    "        Returns: the state of the cell\n",
    "        \"\"\"\n",
    "        return self.boardState[position]\n",
    "\n",
    "    def getBoardState(self):\n",
    "        \"\"\"Get the state of the board\n",
    "        Returns: A dictionary with keys 1 to boardDimensions**2 and values 'X', 'O' or ' '\n",
    "        \"\"\"\n",
    "        return self.boardState\n",
    "\n",
    "    def getBoardDimensions(self):\n",
    "        \"\"\"Get the dimensions of the board\n",
    "        Returns: An integer representing the dimensions of the board \n",
    "        \"\"\"\n",
    "        return self.boardDimensions\n",
    "\n",
    "    def getActions(self, state):\n",
    "        \"\"\"Get all valid actions for a given state.\"\"\"\n",
    "        return [\n",
    "            i\n",
    "            for i in range(1, self.boardDimensions**2 + 1)\n",
    "            if state.getCellState(i) == ' '\n",
    "        ]\n",
    "    \n",
    "    def spaceIsFree(self, position):\n",
    "        if self.boardState[position] == ' ':\n",
    "            return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class GameLogic():\n",
    "    def __init__(self, boardGame):\n",
    "        self.boardGame = boardGame\n",
    "\n",
    "    @abstractmethod\n",
    "    def chkForkWin(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def chkForDraw(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def chkMarkForWin(self, letter):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeGameLogic(GameLogic):\n",
    "    def __init__(self, boardGame):\n",
    "        super().__init__(boardGame)\n",
    "        \"\"\"Create a game logic object for the board game\n",
    "        Args: boardGame (Board): the board game. Must be a subclass of Board\n",
    "        \"\"\"\n",
    "\n",
    "    def chkForDraw(self, state=None):\n",
    "        \"\"\"Check if the game is a draw.\n",
    "        Returns: bool: True if the game is a draw, False otherwise.\n",
    "        \"\"\"\n",
    "        if state is not None:\n",
    "            boardState = state.getBoardState()\n",
    "        else:\n",
    "            boardState = self.boardGame.getBoardState()\n",
    "        return all(boardState[key] != ' ' for key in boardState.keys())\n",
    "\n",
    "    def chkForWin(self, state=None):\n",
    "        \"\"\"Check if any player has won.\n",
    "        Returns: bool: True if any player has won, False otherwise.\n",
    "        \"\"\"\n",
    "        # print('chkForWin')\n",
    "        if state is not None:\n",
    "            boardState = state.getBoardState()\n",
    "            boardDimensions = state.getBoardDimensions()\n",
    "        else:\n",
    "            boardState = self.boardGame.getBoardState()\n",
    "            boardDimensions = self.boardGame.getBoardDimensions()\n",
    "        for i in range(boardDimensions):\n",
    "            row = [boardState[i*boardDimensions+j+1] for j in range(boardDimensions)]\n",
    "            if len(set(row)) == 1 and row[0] != ' ':\n",
    "                return True\n",
    "        for i in range(boardDimensions):\n",
    "            column = [boardState[j*boardDimensions+i+1] for j in range(boardDimensions)]\n",
    "            if len(set(column)) == 1 and column[0] != ' ':\n",
    "                return True\n",
    "        diagonal1 = [boardState[i*boardDimensions+i+1] for i in range(boardDimensions)]\n",
    "        diagonal2 = [boardState[i*boardDimensions+(boardDimensions-i-1)+1] for i in range(boardDimensions)]\n",
    "        if len(set(diagonal1)) == 1 and diagonal1[0] != ' ':\n",
    "            return True\n",
    "        return len(set(diagonal2)) == 1 and diagonal2[0] != ' '\n",
    "\n",
    "    def chkMarkForWin(self, letter, state=None):\n",
    "        \"\"\"Check if the player with the specified letter has won.\n",
    "        Args: letter (str): Letter of the player to check for win.\n",
    "        Returns: bool: True if the player with the specified letter has won, False otherwise.\n",
    "        \"\"\"\n",
    "        if state is not None:\n",
    "            boardState = state.getBoardState()\n",
    "            boardDimensions = state.getBoardDimensions()\n",
    "        else:\n",
    "            boardState = self.boardGame.getBoardState()\n",
    "            boardDimensions = self.boardGame.getBoardDimensions()\n",
    "        # check rows\n",
    "        for i in range(boardDimensions):\n",
    "            row = [boardState[i*boardDimensions+j+1] for j in range(boardDimensions)]\n",
    "            if len(set(row)) == 1 and row[0] == letter:\n",
    "                return True\n",
    "        for i in range(boardDimensions):\n",
    "            column = [boardState[j*boardDimensions+i+1] for j in range(boardDimensions)]\n",
    "            if len(set(column)) == 1 and column[0] == letter:\n",
    "                return True\n",
    "        diagonal1 = [boardState[i*boardDimensions+i+1] for i in range(boardDimensions)]\n",
    "        diagonal2 = [boardState[i*boardDimensions+(boardDimensions-i-1)+1] for i in range(boardDimensions)]\n",
    "        if len(set(diagonal1)) == 1 and diagonal1[0] == letter:\n",
    "            return True\n",
    "        return len(set(diagonal2)) == 1 and diagonal2[0] == letter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Player Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Player(ABC):\n",
    "    def __init__(self, letter, algorithm):\n",
    "        self.letter = letter\n",
    "        self.algorithm = algorithm\n",
    "        \"\"\"Create a player object\n",
    "        Args: letter (str): the letter of the player. Must be 'X' or 'O'\n",
    "        Args: algorithm (Algorithm): the algorithm used by the player. Must be a subclass of Algorithm\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def makeMove(self, boardGame):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanPlayer(Player):\n",
    "    def __init__(self, letter, algorithm):\n",
    "        self.letter = letter\n",
    "        self.algorithm = algorithm\n",
    "    \n",
    "    # function for player to choose a position\n",
    "    def makeMove(self, boardGame):\n",
    "        \"\"\"Make a move by asking for input from the user.\n",
    "        Args: boardGame (Board): The board game object.\n",
    "        If the position is not free, ask for another position.\n",
    "        If the position is free, set the cell state to the player's letter.\n",
    "        \"\"\"\n",
    "        position = self.algorithm.bestMove(boardGame, self.letter)\n",
    "        boardGame.setCellState(position, self.letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputerPlayer(Player):\n",
    "    def __init__(self, letter, algorithm):\n",
    "        self.letter = letter\n",
    "        self.algorithm = algorithm\n",
    "    \n",
    "    def makeMove(self, boardGame):\n",
    "        \"\"\"Make a move by using the algorithm to find the best move.\n",
    "        Args: boardGame (Board): The board game object.\n",
    "        Set the cell state to the player's letter.\n",
    "        \"\"\"\n",
    "        print('Computer is thinking...')\n",
    "        position = self.algorithm.bestMove(boardGame, self.letter)\n",
    "        print(f\"Computer chose position: {position}\")\n",
    "        boardGame.setCellState(position, self.letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Algorithm(ABC):\n",
    "    def __init__(self, boardGame):\n",
    "        self.boardGame = boardGame\n",
    "        \"\"\"Create an algorithm object for the board game\n",
    "        Args: boardGame (Board): the board game. Must be a subclass of Board\n",
    "        \"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def bestMove(self, boardGame, letter):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MiniMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Minimax(Algorithm):\n",
    "    def __init__(self, boardGame):\n",
    "        super().__init__(boardGame)\n",
    "\n",
    "    def bestMove(self, boardGame, letter):\n",
    "        \"\"\"Find the best move for the computer player.\n",
    "        Args: boardGame (Board): The board game object\n",
    "        Args: letter (str): The letter of the computer player.\n",
    "        Returns: int: The position of the best move.\"\"\"\n",
    "        boardState = boardGame.getBoardState()\n",
    "        bestScore = -1000\n",
    "        bestMove = 0\n",
    "        for key in boardState.keys():\n",
    "            if boardState[key] == ' ':\n",
    "                boardState[key] = letter\n",
    "                score = self.minimax(boardState, 0, False, letter)\n",
    "                boardState[key] = ' '\n",
    "                if score > bestScore:\n",
    "                    bestScore = score\n",
    "                    bestMove = key\n",
    "        return bestMove\n",
    "\n",
    "    def minimax(self, boardState, depth, isMaximizing, letter, maxDepth=5):\n",
    "        \"\"\"Find the best score for the computer player.\n",
    "        Args: boardState (dict): The board state.\n",
    "        Args: depth (int): The depth of the tree.\n",
    "        Args: isMaximizing (bool): Whether the player is maximizing or not.\n",
    "        Args: letter (str): The letter of the computer player.\n",
    "        Args: maxDepth (int): The maximum depth of the tree.\n",
    "        Returns: int: The best score for the computer player.\"\"\"\n",
    "        \n",
    "        gameLogic = TicTacToeGameLogic(self.boardGame)\n",
    "        opponentLetter = 'O' if letter == 'X' else 'X'\n",
    "        if(gameLogic.chkMarkForWin(letter)):\n",
    "            return 1\n",
    "        elif(gameLogic.chkMarkForWin(opponentLetter)):\n",
    "            return -1\n",
    "        elif(gameLogic.chkForDraw()):\n",
    "            return 0\n",
    "        elif depth >= maxDepth:\n",
    "            return 0\n",
    "\n",
    "        if isMaximizing:\n",
    "            bestScore = -1000\n",
    "            for key in boardState.keys():\n",
    "                if boardState[key] == ' ':\n",
    "                    boardState[key] = letter\n",
    "                    score = self.minimax(boardState, depth + 1, False, letter)\n",
    "                    boardState[key] = ' '\n",
    "                    if score > bestScore:\n",
    "                        bestScore = score\n",
    "        else:\n",
    "            bestScore = 1000\n",
    "            for key in boardState.keys():\n",
    "                if boardState[key] == ' ':\n",
    "                    boardState[key] = opponentLetter\n",
    "                    score = self.minimax(boardState, depth + 1, True, letter)\n",
    "                    boardState[key] = ' '\n",
    "                    if score < bestScore:\n",
    "                        bestScore = score\n",
    "        return bestScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimax with Alpha-Beta Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimaxAlphaBeta(Algorithm):\n",
    "    def __init__(self, boardGame):\n",
    "        super().__init__(boardGame)\n",
    "\n",
    "    def bestMove(self, boardGame, letter):\n",
    "        boardState = boardGame.getBoardState()\n",
    "        bestScore = -1000\n",
    "        bestMove = 0\n",
    "        for key in boardState.keys():\n",
    "            if boardState[key] == ' ':\n",
    "                boardState[key] = letter\n",
    "                score = self.minimax(boardState, 0, False, letter)\n",
    "                boardState[key] = ' '\n",
    "                if score > bestScore:\n",
    "                    bestScore = score\n",
    "                    bestMove = key\n",
    "        return bestMove\n",
    "\n",
    "    def minimax(self, boardState, depth, isMaximizing, letter, alpha=-1000, beta=1000, maxDepth=5):\n",
    "        \"\"\"Find the best score for the computer player.\n",
    "        Args: boardState (dict): The board state.\n",
    "        Args: depth (int): The depth of the tree.\n",
    "        Args: isMaximizing (bool): Whether the player is maximizing or not.\n",
    "        Args: letter (str): The letter of the computer player.\n",
    "        Args: alpha (int): The alpha value. \n",
    "        Args: beta (int): The beta value.\n",
    "        Args: maxDepth (int): The maximum depth of the tree.\n",
    "        Returns: int: The best score for the computer player.\"\"\"\n",
    "        gameLogic = TicTacToeGameLogic(self.boardGame)\n",
    "        opponentLetter = 'O' if letter == 'X' else 'X'\n",
    "        if(gameLogic.chkMarkForWin(letter)):\n",
    "            return 1\n",
    "        elif(gameLogic.chkMarkForWin(opponentLetter)):\n",
    "            return -1\n",
    "        elif(gameLogic.chkForDraw()):\n",
    "            return 0\n",
    "        elif depth >= maxDepth:\n",
    "            return 0\n",
    "\n",
    "        if isMaximizing:\n",
    "            bestScore = -1000\n",
    "            for key in boardState.keys():\n",
    "                if boardState[key] == ' ':\n",
    "                    boardState[key] = letter\n",
    "                    score = self.minimax(boardState, depth+1, False, letter, alpha, beta)\n",
    "                    boardState[key] = ' '\n",
    "                    bestScore = max(score, bestScore)\n",
    "                    alpha = max(alpha, score)\n",
    "                    if beta <= alpha:\n",
    "                        break\n",
    "        else:\n",
    "            bestScore = 1000\n",
    "            for key in boardState.keys():\n",
    "                if boardState[key] == ' ':\n",
    "                    boardState[key] = opponentLetter\n",
    "                    score = self.minimax(boardState, depth+1, True, letter, alpha, beta)\n",
    "                    boardState[key] = ' '\n",
    "                    bestScore = min(score, bestScore)\n",
    "                    beta = min(beta, score)\n",
    "                    if beta <= alpha:\n",
    "                        break\n",
    "        return bestScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Value Iteration\n",
    "- These classes solve the tic-tac-toe game using the value iteration algorithm.\n",
    "- We assume the tic-tac-toe game is an MDP with states, actions, transition probabilities, and rewards.\n",
    "- States: the board state.\n",
    "- Actions: the position to place the next move.\n",
    "- Transition probabilities: 1.0 if the move is valid, 0.0 otherwise.\n",
    "- Rewards: 1 if the move is a winning move, 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "\n",
    "class ValueIteration(Algorithm):\n",
    "    \"\"\" Value iteration algorithm for Tic Tac Toe\n",
    "    Args: boardGame (Board): The board game object\n",
    "    Initializes the value function, graph, and policy for the board game\"\"\"\n",
    "    \n",
    "    def __init__(self, boardGame):\n",
    "        super().__init__(boardGame)\n",
    "        self.policy =  {}\n",
    "        self.G = Graph()\n",
    "        self.value_function = {}\n",
    "        self.valueIteration()\n",
    "\n",
    "\n",
    "    def bestMove(self, boardGame, letter):\n",
    "        \"\"\"Find the best move for the computer player.\n",
    "        Args: boardGame (Board): The board game object\n",
    "        Args: letter (str): The letter of the computer player.\n",
    "        Returns: int: The position of the best move.\"\"\"\n",
    "\n",
    "        boardState = boardGame.getBoardState()\n",
    "        boardStateTuple = tuple(boardState.values())\n",
    "\n",
    "        if action := self.policy.get(boardStateTuple, None):\n",
    "            return self.extract_policy(action, boardStateTuple, boardState)\n",
    "        else:\n",
    "            print(\"No action found in policy for this state.\")\n",
    "\n",
    "\n",
    "    def extract_policy(self, action, boardStateTuple, boardState):\n",
    "        \"\"\" Extract the policy given a state\n",
    "        Args: action (int): The action to take\n",
    "        Args: boardStateTuple (tuple): The board state as a tuple\n",
    "        Args: boardState (dict): The board state as a dictionary\n",
    "        Returns: int: The position of the best move.\"\"\"\n",
    "\n",
    "        print(f\"Action found in policy: {action}\")\n",
    "\n",
    "        # Print the value of the current state\n",
    "        current_state_value = self.value_function.get(boardStateTuple, \"Unknown\")\n",
    "        print(f\"Value of current state: {current_state_value}\")\n",
    "\n",
    "        # If you can calculate the next state, you can print its value too\n",
    "        state_tuple = tuple(boardState.values())\n",
    "        neighbors = self.G.get_neighbors(state_tuple)\n",
    "        # print all the neighbors\n",
    "        for neighbor in neighbors:\n",
    "            print(f\"Neighbor: {neighbor}\")\n",
    "            print(f\"Value of neighbor: {self.value_function.get(neighbor, 'Unknown')}\")\n",
    "\n",
    "        # Print all possible actions and their values (if you have this information)\n",
    "        # print(f\"All possible actions and their values: {some_dictionary}\")\n",
    "\n",
    "        return action\n",
    "\n",
    "            \n",
    "    def valueIteration(self):\n",
    "        \"\"\" Perform value iteration to find the optimal policy and value function\"\"\"\n",
    "\n",
    "        game_logic = TicTacToeGameLogic(self.boardGame)\n",
    "        best_policy = {}\n",
    "        blankBoard = copy.deepcopy(self.boardGame)\n",
    "        graph = self.initialize(self.G, blankBoard, game_logic)\n",
    "        print(\"Graph initialized\")\n",
    "        self.value_function = {state: 0 for state in graph.nodes}\n",
    "        if self.converge(game_logic):\n",
    "            print(\"Converged\")\n",
    "        # print(f\"Value function: {self.value_function}\")\n",
    "\n",
    "\n",
    "    def initialize(self, graph, board, game_logic):\n",
    "        \"\"\" Initialize the graph with all possible states and actions\n",
    "        Args: graph (Graph): The graph object\n",
    "        Args: board (Board): The board object\n",
    "        Args: game_logic (GameLogic): The game logic object\n",
    "        Returns: Graph: The graph object with all possible states and actions\"\"\"\n",
    "        print(\"Initializing...\")\n",
    "        print(f\"Start state: {board.getBoardState()}\")\n",
    "        graph.add_node(tuple(board.getBoardState().values()))\n",
    "        queue = [(board, 'X')]  # Add the player to the queue\n",
    "        while queue:\n",
    "            state, current_player = queue.pop(0)\n",
    "            for action in board.getActions(state):\n",
    "                new_state = copy.deepcopy(state)\n",
    "                new_state.setCellState(action, current_player)\n",
    "                new_state_tuple = tuple(new_state.getBoardState().values())\n",
    "                \n",
    "                edge_reward = 0  # Default edge reward\n",
    "                \n",
    "                # Check if the new state is a terminal state\n",
    "                if game_logic.chkForWin(new_state):\n",
    "                    graph.add_node(new_state_tuple)\n",
    "                    if game_logic.chkMarkForWin('X', new_state):\n",
    "                        edge_reward = 1\n",
    "                        graph.nodes[new_state_tuple].value = 1\n",
    "                    elif game_logic.chkMarkForWin('O', new_state):\n",
    "                        edge_reward = -1\n",
    "                        graph.nodes[new_state_tuple].value = -1\n",
    "                    graph.add_edge(tuple(state.getBoardState().values()), new_state_tuple, edge_reward)\n",
    "                    continue  # Skip adding this state to the queue\n",
    "                elif game_logic.chkForDraw(new_state):\n",
    "                    graph.add_node(new_state_tuple)\n",
    "                    graph.nodes[new_state_tuple].value = 0\n",
    "                \n",
    "                if new_state_tuple not in graph.nodes:\n",
    "                    graph.add_node(new_state_tuple)\n",
    "                    next_player = 'O' if current_player == 'X' else 'X'\n",
    "                    queue.append((new_state, next_player))\n",
    "                \n",
    "                graph.add_edge(tuple(state.getBoardState().values()), new_state_tuple, edge_reward)\n",
    "                graph.update_edge_reward(tuple(state.getBoardState().values()), new_state_tuple)\n",
    "        \n",
    "        if not graph.nodes:\n",
    "            raise ValueError(\"Graph is empty\")\n",
    "        return graph\n",
    "\n",
    "\n",
    "    def converge(self, game_logic, gamma=0.9, epsilon=1e-4):\n",
    "        \"\"\" Converges the value function and policy\n",
    "        Args: game_logic (GameLogic): The game logic object\n",
    "        Args: gamma (float): The discount factor\n",
    "        Args: epsilon (float): The convergence factor (default 1e-4)\n",
    "        Returns: bool: True if the value function converged, False otherwise\"\"\"\n",
    "        print(\"Converging...\")\n",
    "        delta = float('inf')\n",
    "        i = 0\n",
    "        # Loop until the value function converges\n",
    "        while delta > epsilon:\n",
    "            delta = 0  # Reset delta for each iteration\n",
    "\n",
    "            # Iterate through all states\n",
    "            for curr_state in self.G.nodes.values():\n",
    "                old_value = self.value_function.get(curr_state.state, 0)  # Get the old value function for the state\n",
    "                best_action_value = self.calculateBestAction(curr_state.state, gamma)  # Calculate the best action value\n",
    "                # print(f\"Old value for state {curr_state.state}: {old_value}\")\n",
    "                # print(f\"New value for state {curr_state.state}: {best_action_value}\")\n",
    "\n",
    "                # Update the value function\n",
    "                self.value_function[curr_state.state] = best_action_value\n",
    "\n",
    "                # Update the policy\n",
    "                self.policy[curr_state.state] = self.calculateBestAction(curr_state.state, gamma, return_action=True)\n",
    "\n",
    "                # Calculate the change in the value function for this state\n",
    "                delta = max(delta, abs(old_value - best_action_value))\n",
    "                # Inside the converge method\n",
    "                # print(f\"Delta: {delta}\")  # To check the convergence value\n",
    "\n",
    "            i += 1\n",
    "        print(f\"Number of iterations: {i}\")\n",
    "\n",
    "        print(\"Value function converged.\")\n",
    "\n",
    "\n",
    "    def calculateBestAction(self, state_tuple, gamma, return_action=False):\n",
    "        \"\"\" Calculate the best action for a given state\n",
    "        Args: state_tuple (tuple): The state as a tuple\n",
    "        Args: gamma (float): The discount factor\n",
    "        Args: return_action (bool): Whether to return the action or the value\n",
    "        Returns: float: The value of the best action, or the best action itself\"\"\"\n",
    "\n",
    "        max_value = float('-inf')  # Initialize to negative infinity\n",
    "        best_action = None\n",
    "\n",
    "        # Get the neighbors (possible next states) for the current state\n",
    "        neighbors = self.G.get_neighbors(state_tuple)\n",
    "\n",
    "        # If there are no neighbors, return 0 or None based on the flag\n",
    "        if not neighbors:\n",
    "            self.value_function[state_tuple] = 0  # Update value function for terminal states\n",
    "            return 0 if not return_action else None\n",
    "\n",
    "        # Initialize a list to store the best actions\n",
    "        best_actions = []\n",
    "\n",
    "        # Iterate through each neighbor to find the best action(s)\n",
    "        for neighbor in neighbors:\n",
    "            reward = self.G.get_reward(state_tuple, neighbor)\n",
    "            value = reward + (gamma * self.value_function.get(neighbor, 0))  # Calculate the value of the action\n",
    "\n",
    "            if value > max_value:\n",
    "                max_value = value\n",
    "                best_actions = [self.new_X_position(state_tuple, neighbor) + 1]  # Reset the list with the new best action\n",
    "            elif value == max_value:\n",
    "                best_actions.append(self.new_X_position(state_tuple, neighbor) + 1)  # Add this action to the list of best actions\n",
    "\n",
    "        # Randomly choose one of the best actions\n",
    "        best_action = random.choice(best_actions)\n",
    "\n",
    "        # Update the value function for the current state\n",
    "        self.value_function[state_tuple] = max_value\n",
    "\n",
    "        return max_value if not return_action else best_action\n",
    "\n",
    "    \n",
    "    def new_X_position(self, state_tuple, chosen_neighbor):\n",
    "        \"\"\" Get the position of the new X in the chosen neighbor\n",
    "        Args: state_tuple (tuple): The state as a tuple\n",
    "        Args: chosen_neighbor (tuple): The chosen neighbor as a tuple\n",
    "        Returns: int: The position of the new X in the chosen neighbor\"\"\"\n",
    "        return [\n",
    "            i\n",
    "            for i, (current, next) in enumerate(zip(state_tuple, chosen_neighbor))\n",
    "            if current != next\n",
    "        ][0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_converge():\n",
    "    # Initialize a ValueIteration object with a mock game board and logic\n",
    "    game_factory = TicTacToeCreator(3)\n",
    "    # Test that the value function and policy are empty before convergence\n",
    "    testVI = ValueIteration(game_factory.board)\n",
    "    # Test that the value function and policy are no longer empty\n",
    "    \n",
    "\n",
    "# test_converge()\n",
    "play_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO - Mulitple episodes to find the optimal policy\n",
    "```\n",
    "best_policy = None\n",
    "best_value_function = None\n",
    "highest_average_reward = float('-inf')\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Initialize value function and policy for this episode\n",
    "    value_function = {}\n",
    "    policy = {}\n",
    "    \n",
    "    # Run Value Iteration\n",
    "    value_function, policy = run_value_iteration()\n",
    "    \n",
    "    # Evaluate the policy (you'll need to implement this)\n",
    "    average_reward = evaluate_policy(policy)\n",
    "    \n",
    "    # Update the best policy if this one is better\n",
    "    if average_reward > highest_average_reward:\n",
    "        highest_average_reward = average_reward\n",
    "        best_policy = policy\n",
    "        best_value_function = value_function\n",
    "\n",
    "# best_policy now contains the best policy found\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, state, value):\n",
    "        self.state = state\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"State: {self.state}, Value: {self.value}\"\n",
    "\n",
    "        \n",
    "\n",
    "class Graph:\n",
    "    def __init__(self):\n",
    "        self.nodes = {}\n",
    "        self.edges = {}\n",
    "\n",
    "    def add_node(self, state, value=0):\n",
    "        node = Node(state, value)\n",
    "        self.nodes[state] = node\n",
    "        return node\n",
    "\n",
    "    def add_edge(self, state1, state2, reward=0):\n",
    "        if state1 not in self.nodes:\n",
    "            self.add_node(state1)\n",
    "        if state2 not in self.nodes:\n",
    "            self.add_node(state2)\n",
    "        if state1 not in self.edges:\n",
    "            self.edges[state1] = {}\n",
    "        \n",
    "        # Use the value of the destination node as the reward\n",
    "        reward = self.nodes[state2].value if self.nodes[state2].value != 0 else reward\n",
    "        # print(f\"Reward: {reward}\")\n",
    "        self.edges[state1][state2] = reward\n",
    "\n",
    "    def update_edge_reward(self, state1, state2):\n",
    "        if state1 in self.edges and state2 in self.edges[state1]:\n",
    "            self.edges[state1][state2] = self.nodes[state2].value\n",
    "            # print(f\"Updated edge reward: {self.edges[state1][state2]}\")\n",
    "\n",
    "\n",
    "\n",
    "    def get_neighbors(self, state):\n",
    "        return [] if state not in self.edges else list(self.edges[state].keys())\n",
    "\n",
    "    def get_reward(self, state1, state2):\n",
    "        if state1 not in self.edges or state2 not in self.edges[state1]:\n",
    "            return 0\n",
    "        # print(self.edges[state1][state2])\n",
    "        return self.edges[state1][state2]\n",
    "\n",
    "    def print_graph(self):\n",
    "        for i, node in enumerate(self.nodes.values(), start=1):\n",
    "            print(f\"State {i} Node: {node}\")\n",
    "            neighbors = self.get_neighbors(node.state)\n",
    "            for neighbor in neighbors:\n",
    "                reward = self.get_reward(node.state, neighbor)\n",
    "                print(f\"  -> State {self.get_node_index(neighbor)} (Reward={reward})\")\n",
    "\n",
    "    # def get_node_index(self, state):\n",
    "    #     return list(self.nodes.keys()).index(state) + 1\n",
    "\n",
    "    # def count_winning_states(self):\n",
    "    #     return sum(1 for node in self.nodes.values() if node.value == 1)\n",
    "    \n",
    "    # def count_losing_states(self):\n",
    "    #     return sum(1 for node in self.nodes.values() if node.value == -1)\n",
    "\n",
    "    # def count_draw_states(self):\n",
    "    #     return sum(1 for node in self.nodes.values() if node.value == 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "class QLearning(Algorithm):\n",
    "    def __init__(self, boardGame, explorationRate=0.2, learningRate=0.5, discountFactor=0.9):\n",
    "        super().__init__(boardGame)\n",
    "        self.boardGame = boardGame\n",
    "        self.game_logic = TicTacToeGameLogic(self.boardGame)\n",
    "        self.policy =  {}\n",
    "        self.Qvalues = {}\n",
    "        self.explorationRate = explorationRate\n",
    "        self.learningRate = learningRate\n",
    "        self.discountFactor = discountFactor\n",
    "        \n",
    "        # self.qLearning()\n",
    "        self.train(500)\n",
    "\n",
    "    def export_policy(self) -> dict:\n",
    "        return self.policy\n",
    "\n",
    "\n",
    "    def bestMove(self, boardGame, letter):\n",
    "        \"\"\"Find the best move for the computer player.\n",
    "        Args: boardGame (Board): The board game object\n",
    "        Args: letter (str): The letter of the computer player.\n",
    "        Returns: int: The position of the best move.\"\"\"\n",
    "\n",
    "        boardState = boardGame.getBoardState()\n",
    "        print(f\"Board state: {boardState}\")\n",
    "        boardStateTuple = tuple(boardState.values())\n",
    "        print(f\"Board state tuple: {boardStateTuple}\")\n",
    "        print(f\"Type of boardStateTuple: {type(boardStateTuple)}\")\n",
    "        print(f\"Type of a sample key in policy: {type(next(iter(self.policy))) if self.policy else 'Policy is empty'}\")\n",
    "\n",
    "        if action := self.policy.get(boardStateTuple, None):\n",
    "            return self.extract_policy(action, boardStateTuple, boardState)\n",
    "        print(\"No action found in policy for this state.\")\n",
    "        # Fallback policy: Choose a random available action\n",
    "        possible_actions = [\n",
    "            i + 1\n",
    "            for i in range(boardGame.getBoardDimensions() ** 2)\n",
    "            if boardGame.getCellState(i + 1) == ' '\n",
    "        ]\n",
    "        return random.choice(possible_actions)  # or some heuristic\n",
    "\n",
    "\n",
    "\n",
    "    def extract_policy(self, action, boardStateTuple, boardState):\n",
    "        \"\"\"Extract the best move from the policy.\n",
    "        Args:\n",
    "            action (int): The action suggested by the policy.\n",
    "            boardStateTuple (tuple): The tuple representation of the board state.\n",
    "            boardState (dict): The dictionary representation of the board state.\n",
    "        Returns:\n",
    "            int: The position of the best move.\n",
    "        \"\"\"\n",
    "        if boardState[action] == ' ':\n",
    "            return action\n",
    "    \n",
    "\n",
    "    def qLearning(self):\n",
    "    # Entry point for the Q learning algorithm\n",
    "        current_state = self.boardGame\n",
    "        player_queue = ['X']\n",
    "        while not self.game_logic.chkForWin(current_state) and not self.game_logic.chkForDraw(current_state):\n",
    "            action = self.chooseAction(current_state, self.explorationRate)\n",
    "            # current_state.printBoard()\n",
    "            # print(f\"Action: {action}\")\n",
    "            current_player = player_queue.pop(0)\n",
    "            # print(f\"Current player: {current_player}\")\n",
    "            new_state, next_player = self.takeAction(current_state, action, current_player)\n",
    "            reward = self.observeReward(new_state)  # Call observeReward here\n",
    "            # print(f\"new state: {new_state.getBoardState()}\")\n",
    "            self.updateQValues(current_state, action, reward, new_state)  # Assume you'll implement this method to update Q-values\n",
    "            player_queue.append(next_player)\n",
    "            current_state = new_state\n",
    "            # current_state.printBoard()\n",
    "        # print(\"Game over\")\n",
    "        # for values in self.Qvalues:\n",
    "        #     print(f\"State: {values[0].getBoardState()}, Action: {values[1]}, Q value: {self.Qvalues[values]}\")\n",
    "\n",
    "\n",
    "\n",
    "    def chooseAction(self, current_state, explorationRate):\n",
    "        \"\"\"Choose an action based on the current state and exploration rate\n",
    "        Args: current_state (Board): The current state of the board\n",
    "        Args: explorationRate (float): The exploration rate\n",
    "        Returns: int: The action to take\"\"\"\n",
    "\n",
    "        possible_actions = [\n",
    "            i + 1\n",
    "            for i in range(current_state.getBoardDimensions() ** 2)\n",
    "            if current_state.getCellState(i + 1) == ' '\n",
    "        ]\n",
    "        # print(f\"Possible actions: {possible_actions}\")\n",
    "        random_value = random.uniform(0, 1)\n",
    "        if random_value < explorationRate:\n",
    "            return random.choice(possible_actions)\n",
    "        # negative infinity\n",
    "        max_q_value = float('-inf')\n",
    "        best_action = []\n",
    "        for action in possible_actions:\n",
    "            # print(f\"Action: {action}\")\n",
    "            # if current state, action is not in q values, set it to 0\n",
    "            if (current_state, action) not in self.Qvalues:\n",
    "                # print(f\"State: {current_state.getBoardState()}, Action: {action} not in Q values\")\n",
    "                self.Qvalues[(current_state, action)] = 0\n",
    "                # print(f\"Q value: {self.Qvalues[(current_state, action)]}\")\n",
    "                # get q value for current state, action\n",
    "            q_value = self.Qvalues[(current_state, action)]\n",
    "            if q_value > max_q_value:\n",
    "                max_q_value = q_value\n",
    "                best_action = [action]\n",
    "            elif q_value == max_q_value:\n",
    "                best_action.append(action)\n",
    "        return random.choice(best_action)\n",
    "\n",
    "\n",
    "    def takeAction(self, current_state, action, current_player):\n",
    "\n",
    "        new_state = copy.deepcopy(current_state)\n",
    "        new_state.setCellState(action, current_player)\n",
    "        # swap player\n",
    "        next_player = 'O' if current_player == 'X' else 'X'\n",
    "        return new_state, next_player\n",
    "\n",
    "\n",
    "\n",
    "    def observeReward(self, new_state):\n",
    "        LOSE_REWARD = -1\n",
    "        DRAW_REWARD = 0\n",
    "\n",
    "        if self.game_logic.chkMarkForWin('X', new_state):\n",
    "            # print(\"X wins\")\n",
    "            return 1\n",
    "        elif self.game_logic.chkMarkForWin('O', new_state):\n",
    "            # print(\"O wins\")\n",
    "            return LOSE_REWARD\n",
    "        elif self.game_logic.chkForDraw(new_state):\n",
    "            # print(\"Draw\")\n",
    "            return DRAW_REWARD\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "    def updateQValues(self, current_state, action, reward, new_state):\n",
    "    # Step 1: Get current Q-value\n",
    "        current_q_value = self.Qvalues.get((current_state, action), 0)\n",
    "        # print(f\"Current Q value: {current_q_value}\")\n",
    "        # print(f\"current state: {current_state.getBoardState()}\")\n",
    "\n",
    "        # Step 2: Get max Q-value for new state\n",
    "        possible_actions_new_state = [\n",
    "            i + 1\n",
    "            for i in range(current_state.getBoardDimensions() ** 2)\n",
    "            if current_state.getCellState(i + 1) == ' '\n",
    "        ]\n",
    "\n",
    "        max_new_state_value = max(\n",
    "            self.Qvalues.get((new_state, a), 0) for a in possible_actions_new_state\n",
    "        )\n",
    "\n",
    "        # If new_state is terminal, set max_new_state_value to 0\n",
    "        if self.game_logic.chkForWin(new_state) or self.game_logic.chkForDraw(new_state):\n",
    "            max_new_state_value = 0\n",
    "\n",
    "        # Step 3: Update Q-value using Q-Learning formula\n",
    "        updated_q_value = (1 - self.learningRate) * current_q_value + self.learningRate * (reward + self.discountFactor * max_new_state_value)\n",
    "\n",
    "        # Update the Q-value in the dictionary\n",
    "        self.Qvalues[(current_state, action)] = updated_q_value\n",
    "        # print(f\"Updated Q value: {updated_q_value}\")\n",
    "\n",
    "\n",
    "\n",
    "    def updatePolicy(self):\n",
    "        for state, _ in self.Qvalues.keys():\n",
    "            # Convert the TicTacToeBoard object to a tuple\n",
    "            state_tuple = tuple(state.getBoardState().values())\n",
    "            \n",
    "            possible_actions = [\n",
    "                i + 1\n",
    "                for i in range(state.getBoardDimensions() ** 2)\n",
    "                if state.getCellState(i + 1) == ' '\n",
    "            ]\n",
    "            \n",
    "            # Update the policy using the tuple representation of the state\n",
    "            self.policy[state_tuple] = max(\n",
    "                (\n",
    "                    (action, self.Qvalues.get((state, action), 0))\n",
    "                    for action in possible_actions\n",
    "                ),\n",
    "                key=lambda x: x[1],\n",
    "            )[0]\n",
    "            \n",
    "        # print(\"Policy updated\")\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        for _ in range(num_episodes):\n",
    "            self.qLearning()\n",
    "            self.updatePolicy()\n",
    "        print(\"Training complete\")\n",
    "\n",
    "\n",
    "    def checkConvergence(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# game_factory = TicTacToeCreator(3)\n",
    "# QL = QLearning(game_factory.board)\n",
    "\n",
    "def test_observeReward():\n",
    "    # Initialize QLearning object\n",
    "    game_factory = TicTacToeCreator(2)\n",
    "    ql = QLearning(game_factory.board, 0.5)\n",
    "\n",
    "    # Test 1: 'X' wins\n",
    "    winning_board_for_X = TicTacToeBoard(2)\n",
    "    winning_board_for_X.setCellState(1, 'X')\n",
    "    winning_board_for_X.setCellState(2, 'X')\n",
    "    print(\"Test 1: 'X' wins\")\n",
    "    reward = ql.observeReward(winning_board_for_X)\n",
    "    print(f\"Expected: 1, Got: {reward}\")\n",
    "\n",
    "    # Test 2: 'O' wins\n",
    "    winning_board_for_O = TicTacToeBoard(2)\n",
    "    winning_board_for_O.setCellState(1, 'O')\n",
    "    winning_board_for_O.setCellState(2, 'O')\n",
    "    print(\"Test 2: 'O' wins\")\n",
    "    reward = ql.observeReward(winning_board_for_O)\n",
    "    print(f\"Expected: -1, Got: {reward}\")\n",
    "\n",
    "    # Test 3: Draw\n",
    "    draw_board = TicTacToeBoard(2)\n",
    "    draw_board.setCellState(1, 'X')\n",
    "    draw_board.setCellState(2, 'O')\n",
    "    draw_board.setCellState(3, 'O')\n",
    "    draw_board.setCellState(4, 'X')\n",
    "    print(\"Test 3: Draw\")\n",
    "    reward = ql.observeReward(draw_board)\n",
    "    print(f\"Expected: 0, Got: {reward}\")\n",
    "\n",
    "# Run the test\n",
    "# test_observeReward()\n",
    "\n",
    "    # #print the final policy and Q-values\n",
    "    # print(\"Final policy:\")\n",
    "    # for state, action in self.policy.items():\n",
    "    #     print(f\"State: {state.getBoardState()}, Action: {action}\")\n",
    "    # print(\"Final Q-values:\")\n",
    "    # for values in self.Qvalues:\n",
    "    #     print(f\"State: {values[0].getBoardState()}, Action: {values[1]}, Q value: {self.Qvalues[values]}\")\n",
    "\n",
    "\n",
    "# play_game()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### TODO - Pretrain the Q-Table \n",
    "### Instead of training every time the game is played, we can pretrain the Q-Table and load it when the game starts.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Tree Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "\n",
    "class TreeNode:\n",
    "    \"\"\"\n",
    "    A node in the MCTS tree.\n",
    "    Each node keeps track of its own value Q, prior probability P, and\n",
    "    its visit-count-adjusted prior score u.\n",
    "    \"\"\"\n",
    "    def __init__(self, state: Any, parent: 'TreeNode', visit_count: int, value: float):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.visit_count = visit_count\n",
    "        self.value = value\n",
    "        self.children: List[TreeNode] = []\n",
    "        print(f\"TreeNode initialized with state type: {type(self.state)}, content: {self.state.getBoardState() if self.state else None}\")\n",
    "\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"State: {self.state}, Visit Count: {self.visit_count}, Value: {self.value}\"\n",
    "\n",
    "class Tree:\n",
    "    \"\"\"\n",
    "    A tree in the MCTS algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.root: TreeNode = None\n",
    "\n",
    "    def set_root(self, state: Any):\n",
    "        self.root = TreeNode(state, None, 0, 0)\n",
    "\n",
    "    def add_child(self, parent: TreeNode, state: Any) -> TreeNode:\n",
    "        node = TreeNode(state, parent, 0, 0)\n",
    "        print(f\"Child's parent: {node.parent}\")\n",
    "        parent.children.append(node)\n",
    "\n",
    "        return node\n",
    "\n",
    "    def get_node(self, state: Any) -> TreeNode:\n",
    "        return self._get_node(state, self.root)\n",
    "\n",
    "    def get_state(self, node: TreeNode) -> Any:\n",
    "        # print(f\"Retrieved state type: {type(node.state)}, content: {node.state}\")\n",
    "        return node.state\n",
    "    \n",
    "    def get_root(self) -> TreeNode:\n",
    "        return self.root\n",
    "\n",
    "    def get_children(self, node: TreeNode) -> List[TreeNode]:\n",
    "        # print(f\"Retrieved children type: {type(node.children)}, content: {node.children}\")\n",
    "        return node.children\n",
    "    \n",
    "    def get_parent(self, node: TreeNode) -> TreeNode:\n",
    "        return node.parent\n",
    "\n",
    "    def get_visit_count(self, node: TreeNode) -> int:\n",
    "        return node.visit_count\n",
    "\n",
    "    def get_value(self, node: TreeNode) -> float:\n",
    "        return node.value\n",
    "\n",
    "    def increment_visit_count(self, node: TreeNode):\n",
    "        node.visit_count += 1\n",
    "        \n",
    "    def add_value(self, node: TreeNode, value: float):\n",
    "        node.value = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from collections import deque\n",
    "\n",
    "class MonteCarloTreeSearch(Algorithm):\n",
    "    \"\"\"\n",
    "    Monte Carlo Tree Search algorithm for Tic Tac Toe\n",
    "    \"\"\"\n",
    "    def __init__(self, board_game: Any, q_learning_policy: dict, simulation_limit: int = 100, exploration_constant: float = 1.414):\n",
    "        super().__init__(board_game)\n",
    "        self.tree = Tree()\n",
    "        self.tree.set_root(board_game)\n",
    "        self.game_logic = TicTacToeGameLogic(board_game)\n",
    "        self.policy = q_learning_policy  # Importing the Q-Learning policy\n",
    "        self.q_values = {}\n",
    "        self.simulation_count = 0\n",
    "        self.simulation_limit = simulation_limit\n",
    "        self.exploration_constant = exploration_constant\n",
    "        self.player_queue = deque(['X', 'O'])\n",
    "\n",
    "    def bestMove(self, board_game: Any, letter: str) -> int:\n",
    "        \"\"\" \n",
    "        Use MCTS to find the best move\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def import_ql_policy(self, q_learning_policy: dict):\n",
    "        \"\"\" \n",
    "        Import the Q-Learning policy\n",
    "        \"\"\"\n",
    "        self.policy = q_learning_policy\n",
    "\n",
    "    def select(self):\n",
    "        current_node = self.tree.get_root()  # Start at the root node\n",
    "        while not self.is_leaf_node(current_node):\n",
    "            children = self.tree.get_children(current_node)\n",
    "            # Use UCT to select the best child node to explore\n",
    "            best_child = max(children, key=lambda child: self.uct_value(current_node, child))\n",
    "            current_node = best_child\n",
    "        return current_node  # Return the leaf node for expansion\n",
    "\n",
    "    def uct_value(self, parent, child):\n",
    "        w = self.tree.get_value(child)  # Total reward of the node\n",
    "        n = self.tree.get_visit_count(child)  # Number of times the node has been visited\n",
    "        N = self.tree.get_visit_count(parent)  # Number of times the parent has been visited\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if n == 0:\n",
    "            return float('inf')\n",
    "        \n",
    "        return w / n + self.exploration_constant * ((N / n) ** 0.5)\n",
    "\n",
    "    def is_leaf_node(self, node):\n",
    "        return len(self.tree.get_children(node)) == 0\n",
    "\n",
    "\n",
    "\n",
    "    def expand(self, node):\n",
    "        \"\"\"\n",
    "        Expand the tree by adding new nodes based on possible actions.\n",
    "        \"\"\"\n",
    "        current_state = self.tree.get_state(node)  # Assuming get_state returns a TicTacToeBoard object\n",
    "        print(\"Expanding...\")\n",
    "        # print(f\"Current state in expand: Type - {type(current_state)}, Content - {current_state}\")\n",
    "        \n",
    "        board_dimension = current_state.getBoardDimensions()\n",
    "        \n",
    "        # Get the current player from the queue\n",
    "        current_player = self.player_queue.popleft()\n",
    "        \n",
    "        possible_actions = [\n",
    "            i + 1\n",
    "            for i in range(board_dimension ** 2)\n",
    "            if current_state.getCellState(i + 1) == ' '\n",
    "        ]\n",
    "        \n",
    "        # Before the loop\n",
    "        # print(f\"Type of current_state: {type(current_state)}\")\n",
    "\n",
    "        for action in possible_actions:\n",
    "            new_state = copy.deepcopy(current_state)\n",
    "            \n",
    "            # Debugging setCellState\n",
    "            returned_state = new_state.setCellState(action, current_player)\n",
    "            # print(f\"Type of returned_state: {type(returned_state)}\")\n",
    "            # print(f\"Type of new_state: {type(new_state)}\")\n",
    "            # print(f\"Content of new_state: {new_state.getBoardState()}\")\n",
    "            # print(f\"Type of node: {type(node)}\")\n",
    "            \n",
    "            \n",
    "            # Add the new state as a child node\n",
    "            new_node = self.tree.add_child(node, new_state)\n",
    "            \n",
    "            # print(f\"New node state type: {type(new_node.state)}, content: {new_node.state}\")\n",
    "\n",
    "# Put the current player back at the end of the queue\n",
    "        self.player_queue.append(current_player)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def is_terminal_state(self, state):\n",
    "        return self.game_logic.chkForDraw(state) or self.game_logic.chkForWin(state)\n",
    "\n",
    "\n",
    "    def simulate(self, node):\n",
    "        \"\"\"\n",
    "        Use Q-Learning policy to simulate the game from the given node to a terminal state.\n",
    "        \"\"\"\n",
    "        print(\"Simulating...\")\n",
    "        \n",
    "        # Debug: Print the type and content of the node's state\n",
    "        current_state = self.tree.get_state(node)\n",
    "        print(f\"Current state in simulate: Type - {type(current_state)}, Content - {current_state.getBoardState()}\")\n",
    "        \n",
    "        # Ensure that the node's state is a TicTacToeBoard object\n",
    "        if not isinstance(current_state, TicTacToeBoard):\n",
    "            raise ValueError(\"Node's state must be a TicTacToeBoard object.\")\n",
    "            \n",
    "        while not self.is_terminal_state(current_state):\n",
    "            # Use Q-Learning policy to choose an action\n",
    "            state_str = str(current_state.getBoardState())  # Convert the board state to a string or some hashable form\n",
    "            if state_str in self.policy:\n",
    "                action = max(self.policy[state_str], key=self.policy[state_str].get)  # Choose the action with the highest Q-value\n",
    "            else:\n",
    "                # If the state is not in the policy, choose a random action\n",
    "                possible_actions = [\n",
    "                    i + 1\n",
    "                    for i in range(current_state.getBoardDimensions() ** 2)\n",
    "                    if current_state.getCellState(i + 1) == ' '\n",
    "                ]\n",
    "                action = random.choice(possible_actions)\n",
    "            \n",
    "            # Update the state based on the chosen action\n",
    "            current_player = self.player_queue.popleft()\n",
    "            new_state = copy.deepcopy(current_state)\n",
    "            new_state.setCellState(action, current_player)\n",
    "            self.player_queue.append(current_player)\n",
    "            current_state = new_state\n",
    "            print(f\"Updated state in simulate: Type - {type(current_state)}, Content - {current_state.getBoardState()}\")\n",
    "\n",
    "\n",
    "        # Return the reward of the terminal state (this part will be implemented later)\n",
    "\n",
    "\n",
    "\n",
    "    def backpropagate(self, leaf_node, reward):\n",
    "        current_node = leaf_node  # Start at the leaf node\n",
    "        while current_node is not None:  # Traverse back to the root\n",
    "            self.tree.increment_visit_count(current_node)  # Increment the visit_count of the current node\n",
    "            self.tree.add_value(current_node, reward)  # Update the value of the current node based on the reward\n",
    "\n",
    "            # Check if the current node is the root\n",
    "            if current_node == self.tree.get_root():\n",
    "                break  # Stop backpropagation if you've reached the root\n",
    "\n",
    "            current_node = self.tree.get_parent(current_node)  # Move to the parent node\n",
    "            print(f\"Current node in backpropagate: Type - {type(current_node)}, Content - {current_node.state.getBoardState() if current_node else None}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Optional, Deque\n",
    "from collections import deque\n",
    "\n",
    "class Integration:\n",
    "    def __init__(self, \n",
    "                 mcts_instance: MonteCarloTreeSearch, \n",
    "                 q_learning_instance: QLearning, \n",
    "                 board_game: Any, \n",
    "                 simulation_limit: int = 100, \n",
    "                 exploration_constant: float = 1.414):\n",
    "        \"\"\"\n",
    "        Initialize the integration class with instances of MonteCarloTreeSearch and QLearning.\n",
    "        \n",
    "        Parameters:\n",
    "            mcts_instance: An instance of MonteCarloTreeSearch.\n",
    "            q_learning_instance: An instance of QLearning.\n",
    "            board_game: The initial board state.\n",
    "            simulation_limit: The limit for MCTS simulations.\n",
    "            exploration_constant: The exploration constant for MCTS.\n",
    "        \"\"\"\n",
    "        self.mcts = mcts_instance\n",
    "        self.q_learning = q_learning_instance\n",
    "        self.board_game = board_game\n",
    "        self.simulation_limit = simulation_limit\n",
    "        self.exploration_constant = exploration_constant\n",
    "        self.player_queue: Deque[str] = deque(['X', 'O'])\n",
    "\n",
    "    def initialize(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the mcts tree with the root as the current board state.\n",
    "        \"\"\"\n",
    "        self.mcts.tree.set_root(self.board_game)\n",
    "        print(\"initialized\")\n",
    "\n",
    "    def action_selection(self) -> Any:\n",
    "        \"\"\"\n",
    "        Use Q-Learning policy to select an action, but use MCTS for states where q-learning is uncertain.\n",
    "        \n",
    "        Returns:\n",
    "            The selected action.\n",
    "        \"\"\"\n",
    "        # Step 1: Retrieve Current State\n",
    "        current_state = self.board_game.getBoardState()\n",
    "\n",
    "        # Step 2: Q-Learning Action\n",
    "        q_learning_action = self.q_learning.bestMove(self.board_game, self.player_queue[0])  # Assuming 'X' is the current player\n",
    "\n",
    "        if is_uncertain := self.check_uncertainty(\n",
    "            current_state, q_learning_action\n",
    "        ):\n",
    "            mcts_action = self.mcts_action(current_state)\n",
    "            return mcts_action\n",
    "\n",
    "        # Step 5: Return Action\n",
    "        return q_learning_action\n",
    "\n",
    "    def check_uncertainty(self, current_state, q_learning_action) -> bool:\n",
    "        \"\"\"\n",
    "        Check if Q-Learning is uncertain about the chosen action.\n",
    "        \n",
    "        Args:\n",
    "            current_state: The current board state.\n",
    "            q_learning_action: The action chosen by Q-Learning.\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if uncertain, False otherwise.\n",
    "        \"\"\"\n",
    "        # Step 1: Retrieve Q-Values\n",
    "        possible_actions = [\n",
    "            i + 1\n",
    "            for i in range(self.board_game.getBoardDimensions() ** 2)\n",
    "            if self.board_game.getCellState(i + 1) == ' '\n",
    "        ]\n",
    "        q_values = [self.q_learning.Qvalues.get((current_state, action), 0) for action in possible_actions]\n",
    "        \n",
    "        # Step 2: Check for Multiple Maxima\n",
    "        max_q_value = max(q_values)\n",
    "        count_max_q_value = q_values.count(max_q_value)\n",
    "        \n",
    "        # Step 3: Return Uncertainty Status\n",
    "        return count_max_q_value > 1\n",
    "\n",
    "\n",
    "    def mcts_action(self, current_state) -> Any:\n",
    "        # TODO: Implement logic to use MCTS for action selection\n",
    "        pass\n",
    "\n",
    "\n",
    "    def tree_update(self) -> None:\n",
    "        \"\"\"\n",
    "        Update the tree based on the latest action and state.\n",
    "        \"\"\"\n",
    "        # TODO: Implement tree update logic\n",
    "        pass\n",
    "\n",
    "    def policy_update(self) -> None:\n",
    "        \"\"\"\n",
    "        Update the policy based on the latest action and state.\n",
    "        \"\"\"\n",
    "        # TODO: Implement policy update logic\n",
    "        pass\n",
    "\n",
    "    def efficiency_guide(self) -> Optional[List[Any]]:\n",
    "        \"\"\"\n",
    "        Provide guidance or metrics for improving the efficiency of the algorithms.\n",
    "        \n",
    "        Returns:\n",
    "            A list of suggestions or metrics, or None.\n",
    "        \"\"\"\n",
    "        # TODO: Implement efficiency guide logic\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TicTacToeCreator and QLearning instances\n",
    "game_factory = TicTacToeCreator(3)\n",
    "initial_board = game_factory.board  # Assuming this gives you the initial board\n",
    "q_learning_instance = QLearning(initial_board, 0.5)\n",
    "q_learning_policy = q_learning_instance.export_policy()\n",
    "\n",
    "# Initialize the MonteCarloTreeSearch instance\n",
    "mcts_instance = MonteCarloTreeSearch(initial_board, q_learning_policy)\n",
    "\n",
    "# Initialize the Integration instance\n",
    "integration_instance = Integration(mcts_instance, q_learning_instance, initial_board)\n",
    "\n",
    "# Initialize the Integration class\n",
    "integration_instance.initialize()\n",
    "\n",
    "print(\"Done\")\n",
    "\n",
    "\n",
    "# Certain Scenario\n",
    "# current_state_1 = TicTacToeBoard(3)\n",
    "# current_state_1.setCellState(1, 'X')\n",
    "# current_state_1.setCellState(2, 'O')\n",
    "# current_state_1.setCellState(3, 'X')\n",
    "# current_state_1.setCellState(4, 'O')\n",
    "# current_state_1.setCellState(5, 'X')\n",
    "# current_state_1.setCellState(6, 'O')\n",
    "# current_state_1.setCellState(7, 'X')\n",
    "# current_state_1.setCellState(8, 'O')\n",
    "# current_state_1.setCellState(9, ' ')\n",
    "# action_1 = 9  # The only empty cell\n",
    "# current_state_2 = TicTacToeBoard(3)\n",
    "# current_state_2.setCellState(1, 'X')\n",
    "# current_state_2.setCellState(2, 'X')\n",
    "# current_state_2.setCellState(3, 'X')\n",
    "# current_state_2.setCellState(4, 'O')\n",
    "# current_state_2.setCellState(5, 'O')\n",
    "# current_state_2.setCellState(6, 'O')\n",
    "# current_state_2.setCellState(7, ' ')\n",
    "# current_state_2.setCellState(8, ' ')\n",
    "# current_state_2.setCellState(9, ' ')\n",
    "# action_2 = 7  # One of the empty cells\n",
    "# current_state_3 = TicTacToeBoard(3)\n",
    "# current_state_3.setCellState(5, 'X')\n",
    "# current_state_3.setCellState(1, ' ')\n",
    "# current_state_3.setCellState(2, ' ')\n",
    "# current_state_3.setCellState(3, ' ')\n",
    "# current_state_3.setCellState(4, ' ')\n",
    "# current_state_3.setCellState(6, ' ')\n",
    "# current_state_3.setCellState(7, ' ')\n",
    "# current_state_3.setCellState(8, ' ')\n",
    "# current_state_3.setCellState(9, ' ')\n",
    "# action_3 = 1  # One of the empty cells\n",
    "\n",
    "# # Manually set Q-values for testing\n",
    "# integration_instance.q_learning.Qvalues[(current_state_1, action_1)] = 0.5\n",
    "# integration_instance.q_learning.Qvalues[(current_state_1, action_2)] = 0.3\n",
    "# # Uncertain Scenario\n",
    "# integration_instance.q_learning.Qvalues[(current_state_2, action_1)] = 0.5\n",
    "# integration_instance.q_learning.Qvalues[(current_state_2, action_2)] = 0.5\n",
    "\n",
    "# # Test check_uncertainty method\n",
    "# # Certain Scenario\n",
    "# assert not integration_instance.check_uncertainty(current_state_1, action_1), \"Failed: Expected False for certain scenario\"\n",
    "# # Uncertain Scenario\n",
    "# assert integration_instance.check_uncertainty(current_state_2, action_1), \"Failed: Expected True for uncertain scenario\"\n",
    "\n",
    "# print(\"All tests passed!\")\n",
    "\n",
    "\n",
    "\n",
    "# # Get the root node\n",
    "# root = mcts_instance.tree.get_root()\n",
    "\n",
    "# # Use the select method to get the leaf node\n",
    "# leaf_node = mcts_instance.select()\n",
    "# print(f\"Leaf node: {leaf_node}\")\n",
    "\n",
    "# # Expand the leaf node to add child nodes\n",
    "# mcts_instance.expand(leaf_node)\n",
    "\n",
    "# # Run the simulate method starting from the leaf node\n",
    "# mcts_instance.simulate(leaf_node)\n",
    "\n",
    "# # Manually set a reward for testing\n",
    "# reward = 1\n",
    "\n",
    "# if leaf_node == mcts_instance.tree.get_root():\n",
    "#     print(\"Leaf node is the root.\")\n",
    "# # Call backpropagate (assuming you've implemented it)\n",
    "# mcts_instance.backpropagate(leaf_node, reward)\n",
    "\n",
    "# # Check Updates\n",
    "# assert mcts_instance.tree.get_visit_count(leaf_node) == 1\n",
    "# assert mcts_instance.tree.get_value(leaf_node) == reward\n",
    "\n",
    "# parent_node = mcts_instance.tree.get_parent(leaf_node)\n",
    "# if parent_node is not None:\n",
    "#     assert mcts_instance.tree.get_visit_count(parent_node) == 1\n",
    "#     assert mcts_instance.tree.get_value(parent_node) == reward\n",
    "# else:\n",
    "#     print(\"Parent node is None, likely because the leaf node is the root.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm for the human player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserInput(Algorithm):\n",
    "    def __init__(self, boardGame):\n",
    "        super().__init__(boardGame)\n",
    "\n",
    "    def bestMove(self, boardGame, letter):\n",
    "        \"\"\"Ask the user for input.\n",
    "        Args: boardGame (Board): The board game object.\n",
    "        Args: letter (str): The letter of the computer player.\n",
    "        Returns: int: The position of the user's input.\"\"\"\n",
    "        while True:\n",
    "            try:\n",
    "                position = int(input(\"Please enter a position: \"))\n",
    "                if position < 1 or position > boardGame.getBoardDimensions()**2:\n",
    "                    raise ValueError\n",
    "                if boardGame.spaceIsFree(position):\n",
    "                    return position\n",
    "                else:\n",
    "                    raise ValueError\n",
    "            except ValueError:\n",
    "                print(\"Invalid input!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractGameFactory(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def createGameLogic(self) -> GameLogic:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def createPlayer(self) -> Player:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def createAlgorithm(self) -> Algorithm:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeCreator(AbstractGameFactory):\n",
    "    def __init__(self, dimensions):\n",
    "        self.board = TicTacToeBoard(dimensions)\n",
    "\n",
    "    def createGameLogic(self) -> GameLogic:\n",
    "        \"\"\"Create a game logic object for the board game\n",
    "        Returns: gameLogic (GameLogic): the game logic object. Must be a subclass of GameLogic\n",
    "        \"\"\"\n",
    "        return TicTacToeGameLogic(self.board)\n",
    "\n",
    "    def createPlayer(self, letter, isComputer, algorithm) -> Player:\n",
    "        \"\"\"Create a player object\n",
    "        Args: letter (str): the letter of the player. Must be 'X' or 'O'\n",
    "        Args: isComputer (bool): whether the player is a computer or not\n",
    "        Args: algorithm (Algorithm): the algorithm used by the player. Must be a subclass of Algorithm\n",
    "        Returns: player (Player): the player object. Must be a subclass of Player\n",
    "        \"\"\"\n",
    "        if isComputer:\n",
    "            return ComputerPlayer(letter, algorithm)\n",
    "        return HumanPlayer(letter, algorithm)\n",
    "\n",
    "    def createAlgorithm(self, algorithm) -> Algorithm:\n",
    "        \"\"\"Create an algorithm object for the board game\n",
    "        Args: algorithm (int): the algorithm used by the player. Must be 1, 2, 3 or 4\n",
    "        Returns: algorithm (Algorithm): the algorithm object. Must be a subclass of Algorithm\n",
    "        \"\"\"\n",
    "        if algorithm == 1:\n",
    "            return Minimax(self.board)\n",
    "        elif algorithm == 2:\n",
    "            return MinimaxAlphaBeta(self.board)\n",
    "        elif algorithm == 3:\n",
    "            return ValueIteration(self.board)\n",
    "        elif algorithm == 4:\n",
    "            return QLearning(self.board)\n",
    "        elif algorithm == 5:\n",
    "            return UserInput(self.board)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def prompt_user(prompt, options):\n",
    "    \"\"\"Prompt the user to choose an option.\n",
    "    Args: prompt (str): The prompt to display to the user.\n",
    "    Args: options (list): The list of options to display to the user.\n",
    "    Returns: int: The option chosen by the user.\n",
    "    \"\"\"\n",
    "    print(prompt)\n",
    "    for i, option in enumerate(options):\n",
    "        print(f\"{i+1}. {option}\")\n",
    "    choice = int(input(\"Please choose an option: \"))\n",
    "    while choice < 1 or choice > len(options):\n",
    "        print(f\"Choice must be between 1 and {len(options)}!\")\n",
    "        choice = int(input(\"Please choose an option: \"))\n",
    "    return choice\n",
    "\n",
    "def choose_game():\n",
    "    \"\"\"Prompt the user to choose a game.\n",
    "    Returns: game_factory (AbstractGameFactory): The game factory for the game chosen by the user.\n",
    "    \"\"\"\n",
    "    prompt = \"Welcome to the Game Factory!\\nPlease choose a game:\"\n",
    "    options = [\"Tic Tac Toe\", \"Chess\", \"Backgammon\"]\n",
    "    choice = prompt_user(prompt, options)\n",
    "    if choice == 1:\n",
    "        dimensions = int(input(\"Please enter the board dimensions: \"))\n",
    "        return TicTacToeCreator(dimensions)\n",
    "    elif choice == 2:\n",
    "        return ChessCreator()\n",
    "    elif choice == 3:\n",
    "        return BackgammonCreator()\n",
    "\n",
    "def choose_algorithm():\n",
    "    \"\"\"Prompt the user to choose an algorithm.\n",
    "    Returns: int: The algorithm chosen by the user.\n",
    "    \"\"\"\n",
    "    prompt = \"Which Algorithm should player use?\"\n",
    "    options = [\"Minimax\", \"Minimax with Alpha Beta Pruning\", \"Value Iteration\", \"Q-Learning\", \"User Input\"]\n",
    "    return prompt_user(prompt, options)\n",
    "\n",
    "def create_player(game_factory, letter):\n",
    "    \"\"\"Create a player object\n",
    "    Args: game_factory (AbstractGameFactory): The game factory for the game chosen by the user.\n",
    "    Args: letter (str): the letter of the player. Must be 'X' or 'O'\n",
    "    Returns: player (Player): the player object. Must be a subclass of Player\n",
    "    \"\"\"\n",
    "    is_computer = input(f\"Is player {letter} a computer? (Y/N): \")\n",
    "    while is_computer not in ['Y', 'N']:\n",
    "        print(\"Invalid input!\")\n",
    "        is_computer = input(f\"Is player {letter} a computer? (Y/N): \")\n",
    "    algorithm_choice = choose_algorithm()\n",
    "    algorithm = game_factory.createAlgorithm(algorithm_choice)\n",
    "    return game_factory.createPlayer(letter, is_computer == 'Y', algorithm)\n",
    "\n",
    "def play_game():\n",
    "    \"\"\"Play the game.\"\"\"\n",
    "    game_factory = choose_game()\n",
    "    game_logic = game_factory.createGameLogic()\n",
    "    player_one = create_player(game_factory, 'X')\n",
    "    player_two = create_player(game_factory, 'O')\n",
    "    game_factory.board.printBoard()\n",
    "    start = time.time()\n",
    "    while not game_logic.chkForWin() and not game_logic.chkForDraw():\n",
    "        player_one.makeMove(game_factory.board)\n",
    "        game_factory.board.printBoard()\n",
    "        if game_logic.chkForWin():\n",
    "            print(\"Player\", player_one.letter, \"wins!\")\n",
    "            end = time.time()\n",
    "            print(\"Time taken: \", end - start)\n",
    "            break\n",
    "        elif game_logic.chkForDraw():\n",
    "            print(\"It's a draw!\")\n",
    "            end = time.time()\n",
    "            print(\"Time taken: \", end - start)\n",
    "            break\n",
    "        player_two.makeMove(game_factory.board)\n",
    "        game_factory.board.printBoard()\n",
    "        if game_logic.chkForWin():\n",
    "            print(\"Player\", player_two.letter, \"wins!\")\n",
    "            end = time.time()\n",
    "            print(\"Time taken: \", end - start)\n",
    "            break\n",
    "        elif game_logic.chkForDraw():\n",
    "            print(\"It's a draw!\")\n",
    "            end = time.time()\n",
    "            print(\"Time taken: \", end - start)\n",
    "            break\n",
    "\n",
    "play_game()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
