{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIT320 Advanced Algorithms\n",
    "## Module 12 - MDP and Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Board Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Board(ABC):\n",
    "    def __init__(self, boardDimensions):\n",
    "        self.boardDimensions = boardDimensions\n",
    "\n",
    "    @abstractmethod\n",
    "    def createBoard(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def printBoard(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def setCellState(self, position, state):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def getCellState(self, position):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def getBoardState(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def getBoardDimensions(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def spaceIsFree(self, position):\n",
    "        pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "class TicTacToeBoard(Board):\n",
    "    def __init__(self, boardDimensions):\n",
    "        \"\"\"Create a board of dimensions boardDimensions x boardDimensions\n",
    "        Args: boardDimensions (int): the dimensions of the board\n",
    "        \"\"\"\n",
    "        self.boardDimensions = boardDimensions\n",
    "        self.createBoard()\n",
    "\n",
    "\n",
    "    def createBoard(self):\n",
    "        \"\"\"Create a board of dimensions boardDimensions x boardDimensions\"\"\"\n",
    "        self.boardState = {i+1: ' ' for i in range(self.getBoardDimensions()**2)}\n",
    "\n",
    "    def printBoard(self):\n",
    "        for i in range(self.boardDimensions):\n",
    "            row = [self.boardState[i*self.boardDimensions+j+1] for j in range(self.boardDimensions)]\n",
    "            print('|'.join(row))\n",
    "            if i < self.boardDimensions-1:\n",
    "                print('-'*(self.boardDimensions*2-1))\n",
    "        print('\\n')\n",
    "\n",
    "    def setCellState(self, position, state):\n",
    "        \"\"\"Set the state of a cell on the board\n",
    "        Args:\n",
    "            position (int): the position of the cell\n",
    "            state (str): the state of the cell\n",
    "        \"\"\"\n",
    "        self.boardState[position] = state\n",
    "\n",
    "    def getCellState(self, position):\n",
    "        \"\"\"Get the state of a cell on the board\n",
    "        Args:\n",
    "            position (int): the position of the cell\n",
    "        Returns: the state of the cell\n",
    "        \"\"\"\n",
    "        return self.boardState[position]\n",
    "\n",
    "    def getBoardState(self):\n",
    "        \"\"\"Get the state of the board\n",
    "        Returns: A dictionary with keys 1 to boardDimensions**2 and values 'X', 'O' or ' '\n",
    "        \"\"\"\n",
    "        return self.boardState\n",
    "\n",
    "    def getBoardDimensions(self):\n",
    "        \"\"\"Get the dimensions of the board\n",
    "        Returns: An integer representing the dimensions of the board \n",
    "        \"\"\"\n",
    "        return self.boardDimensions\n",
    "\n",
    "    def getActions(self, state):\n",
    "        \"\"\"Get all valid actions for a given state.\"\"\"\n",
    "        return [\n",
    "            i\n",
    "            for i in range(1, self.boardDimensions**2 + 1)\n",
    "            if state.getCellState(i) == ' '\n",
    "        ]\n",
    "    \n",
    "    def spaceIsFree(self, position):\n",
    "        if self.boardState[position] == ' ':\n",
    "            return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class GameLogic():\n",
    "    def __init__(self, boardGame):\n",
    "        self.boardGame = boardGame\n",
    "\n",
    "    @abstractmethod\n",
    "    def chkForkWin(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def chkForDraw(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def chkMarkForWin(self, letter):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeGameLogic(GameLogic):\n",
    "    def __init__(self, boardGame):\n",
    "        super().__init__(boardGame)\n",
    "        \"\"\"Create a game logic object for the board game\n",
    "        Args: boardGame (Board): the board game. Must be a subclass of Board\n",
    "        \"\"\"\n",
    "\n",
    "    def chkForDraw(self, state=None):\n",
    "        \"\"\"Check if the game is a draw.\n",
    "        Returns: bool: True if the game is a draw, False otherwise.\n",
    "        \"\"\"\n",
    "        if state is not None:\n",
    "            boardState = state.getBoardState()\n",
    "        else:\n",
    "            boardState = self.boardGame.getBoardState()\n",
    "        return all(boardState[key] != ' ' for key in boardState.keys())\n",
    "\n",
    "    def chkForWin(self, state=None):\n",
    "        \"\"\"Check if any player has won.\n",
    "        Returns: bool: True if any player has won, False otherwise.\n",
    "        \"\"\"\n",
    "        # print('chkForWin')\n",
    "        if state is not None:\n",
    "            boardState = state.getBoardState()\n",
    "            boardDimensions = state.getBoardDimensions()\n",
    "        else:\n",
    "            boardState = self.boardGame.getBoardState()\n",
    "            boardDimensions = self.boardGame.getBoardDimensions()\n",
    "        for i in range(boardDimensions):\n",
    "            row = [boardState[i*boardDimensions+j+1] for j in range(boardDimensions)]\n",
    "            if len(set(row)) == 1 and row[0] != ' ':\n",
    "                return True\n",
    "        for i in range(boardDimensions):\n",
    "            column = [boardState[j*boardDimensions+i+1] for j in range(boardDimensions)]\n",
    "            if len(set(column)) == 1 and column[0] != ' ':\n",
    "                return True\n",
    "        diagonal1 = [boardState[i*boardDimensions+i+1] for i in range(boardDimensions)]\n",
    "        diagonal2 = [boardState[i*boardDimensions+(boardDimensions-i-1)+1] for i in range(boardDimensions)]\n",
    "        if len(set(diagonal1)) == 1 and diagonal1[0] != ' ':\n",
    "            return True\n",
    "        return len(set(diagonal2)) == 1 and diagonal2[0] != ' '\n",
    "\n",
    "    def chkMarkForWin(self, letter, state=None):\n",
    "        \"\"\"Check if the player with the specified letter has won.\n",
    "        Args: letter (str): Letter of the player to check for win.\n",
    "        Returns: bool: True if the player with the specified letter has won, False otherwise.\n",
    "        \"\"\"\n",
    "        if state is not None:\n",
    "            boardState = state.getBoardState()\n",
    "            boardDimensions = state.getBoardDimensions()\n",
    "        else:\n",
    "            boardState = self.boardGame.getBoardState()\n",
    "            boardDimensions = self.boardGame.getBoardDimensions()\n",
    "        # check rows\n",
    "        for i in range(boardDimensions):\n",
    "            row = [boardState[i*boardDimensions+j+1] for j in range(boardDimensions)]\n",
    "            if len(set(row)) == 1 and row[0] == letter:\n",
    "                return True\n",
    "        for i in range(boardDimensions):\n",
    "            column = [boardState[j*boardDimensions+i+1] for j in range(boardDimensions)]\n",
    "            if len(set(column)) == 1 and column[0] == letter:\n",
    "                return True\n",
    "        diagonal1 = [boardState[i*boardDimensions+i+1] for i in range(boardDimensions)]\n",
    "        diagonal2 = [boardState[i*boardDimensions+(boardDimensions-i-1)+1] for i in range(boardDimensions)]\n",
    "        if len(set(diagonal1)) == 1 and diagonal1[0] == letter:\n",
    "            return True\n",
    "        return len(set(diagonal2)) == 1 and diagonal2[0] == letter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Player Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Player(ABC):\n",
    "    def __init__(self, letter, algorithm):\n",
    "        self.letter = letter\n",
    "        self.algorithm = algorithm\n",
    "        \"\"\"Create a player object\n",
    "        Args: letter (str): the letter of the player. Must be 'X' or 'O'\n",
    "        Args: algorithm (Algorithm): the algorithm used by the player. Must be a subclass of Algorithm\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def makeMove(self, boardGame):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanPlayer(Player):\n",
    "    def __init__(self, letter, algorithm):\n",
    "        self.letter = letter\n",
    "        self.algorithm = algorithm\n",
    "    \n",
    "    # function for player to choose a position\n",
    "    def makeMove(self, boardGame):\n",
    "        \"\"\"Make a move by asking for input from the user.\n",
    "        Args: boardGame (Board): The board game object.\n",
    "        If the position is not free, ask for another position.\n",
    "        If the position is free, set the cell state to the player's letter.\n",
    "        \"\"\"\n",
    "        position = self.algorithm.bestMove(boardGame, self.letter)\n",
    "        boardGame.setCellState(position, self.letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputerPlayer(Player):\n",
    "    def __init__(self, letter, algorithm):\n",
    "        self.letter = letter\n",
    "        self.algorithm = algorithm\n",
    "    \n",
    "    def makeMove(self, boardGame):\n",
    "        \"\"\"Make a move by using the algorithm to find the best move.\n",
    "        Args: boardGame (Board): The board game object.\n",
    "        Set the cell state to the player's letter.\n",
    "        \"\"\"\n",
    "        print('Computer is thinking...')\n",
    "        position = self.algorithm.bestMove(boardGame, self.letter)\n",
    "        print(f\"Computer chose position: {position}\")\n",
    "        boardGame.setCellState(position, self.letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserInput(Algorithm):\n",
    "    def __init__(self, boardGame):\n",
    "        super().__init__(boardGame)\n",
    "\n",
    "    def bestMove(self, boardGame, letter):\n",
    "        \"\"\"Ask the user for input.\n",
    "        Args: boardGame (Board): The board game object.\n",
    "        Args: letter (str): The letter of the computer player.\n",
    "        Returns: int: The position of the user's input.\"\"\"\n",
    "        while True:\n",
    "            try:\n",
    "                position = int(input(\"Please enter a position: \"))\n",
    "                if position < 1 or position > boardGame.getBoardDimensions()**2:\n",
    "                    raise ValueError\n",
    "                if boardGame.spaceIsFree(position):\n",
    "                    return position\n",
    "                else:\n",
    "                    raise ValueError\n",
    "            except ValueError:\n",
    "                print(\"Invalid input!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game Factory Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractGameFactory(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def createGameLogic(self) -> GameLogic:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def createPlayer(self) -> Player:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def createAlgorithm(self) -> Algorithm:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeCreator(AbstractGameFactory):\n",
    "    def __init__(self, dimensions):\n",
    "        self.board = TicTacToeBoard(dimensions)\n",
    "\n",
    "    def createGameLogic(self) -> GameLogic:\n",
    "        \"\"\"Create a game logic object for the board game\n",
    "        Returns: gameLogic (GameLogic): the game logic object. Must be a subclass of GameLogic\n",
    "        \"\"\"\n",
    "        return TicTacToeGameLogic(self.board)\n",
    "\n",
    "    def createPlayer(self, letter, isComputer, algorithm) -> Player:\n",
    "        \"\"\"Create a player object\n",
    "        Args: letter (str): the letter of the player. Must be 'X' or 'O'\n",
    "        Args: isComputer (bool): whether the player is a computer or not\n",
    "        Args: algorithm (Algorithm): the algorithm used by the player. Must be a subclass of Algorithm\n",
    "        Returns: player (Player): the player object. Must be a subclass of Player\n",
    "        \"\"\"\n",
    "        if isComputer:\n",
    "            return ComputerPlayer(letter, algorithm)\n",
    "        return HumanPlayer(letter, algorithm)\n",
    "\n",
    "    def createAlgorithm(self, algorithm) -> Algorithm:\n",
    "        \"\"\"Create an algorithm object for the board game\n",
    "        Args: algorithm (int): the algorithm used by the player. Must be 1, 2, 3 or 4\n",
    "        Returns: algorithm (Algorithm): the algorithm object. Must be a subclass of Algorithm\n",
    "        \"\"\"\n",
    "        if algorithm == 1:\n",
    "            return Minimax(self.board)\n",
    "        elif algorithm == 2:\n",
    "            return MinimaxAlphaBeta(self.board)\n",
    "        elif algorithm == 3:\n",
    "            return ValueIteration(self.board)\n",
    "        elif algorithm == 4:\n",
    "            return QLearning(self.board)\n",
    "        elif algorithm == 5:\n",
    "            q_learning_policy = model_manager.load_model(dimensions)\n",
    "            return MonteCarloTreeSearch(self.board, q_learning_policy)\n",
    "        elif algorithm == 6:\n",
    "            return UserInput(self.board)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Algorithm(ABC):\n",
    "    def __init__(self, boardGame):\n",
    "        self.boardGame = boardGame\n",
    "        \"\"\"Create an algorithm object for the board game\n",
    "        Args: boardGame (Board): the board game. Must be a subclass of Board\n",
    "        \"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def bestMove(self, boardGame, letter):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MiniMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Minimax(Algorithm):\n",
    "    def __init__(self, boardGame):\n",
    "        super().__init__(boardGame)\n",
    "\n",
    "    def bestMove(self, boardGame, letter):\n",
    "        \"\"\"Find the best move for the computer player.\n",
    "        Args: boardGame (Board): The board game object\n",
    "        Args: letter (str): The letter of the computer player.\n",
    "        Returns: int: The position of the best move.\"\"\"\n",
    "        boardState = boardGame.getBoardState()\n",
    "        bestScore = -1000\n",
    "        bestMove = 0\n",
    "        for key in boardState.keys():\n",
    "            if boardState[key] == ' ':\n",
    "                boardState[key] = letter\n",
    "                score = self.minimax(boardState, 0, False, letter)\n",
    "                boardState[key] = ' '\n",
    "                if score > bestScore:\n",
    "                    bestScore = score\n",
    "                    bestMove = key\n",
    "        return bestMove\n",
    "\n",
    "    def minimax(self, boardState, depth, isMaximizing, letter, maxDepth=5):\n",
    "        \"\"\"Find the best score for the computer player.\n",
    "        Args: boardState (dict): The board state.\n",
    "        Args: depth (int): The depth of the tree.\n",
    "        Args: isMaximizing (bool): Whether the player is maximizing or not.\n",
    "        Args: letter (str): The letter of the computer player.\n",
    "        Args: maxDepth (int): The maximum depth of the tree.\n",
    "        Returns: int: The best score for the computer player.\"\"\"\n",
    "        \n",
    "        gameLogic = TicTacToeGameLogic(self.boardGame)\n",
    "        opponentLetter = 'O' if letter == 'X' else 'X'\n",
    "        if(gameLogic.chkMarkForWin(letter)):\n",
    "            return 1\n",
    "        elif(gameLogic.chkMarkForWin(opponentLetter)):\n",
    "            return -1\n",
    "        elif(gameLogic.chkForDraw()):\n",
    "            return 0\n",
    "        elif depth >= maxDepth:\n",
    "            return 0\n",
    "\n",
    "        if isMaximizing:\n",
    "            bestScore = -1000\n",
    "            for key in boardState.keys():\n",
    "                if boardState[key] == ' ':\n",
    "                    boardState[key] = letter\n",
    "                    score = self.minimax(boardState, depth + 1, False, letter)\n",
    "                    boardState[key] = ' '\n",
    "                    if score > bestScore:\n",
    "                        bestScore = score\n",
    "        else:\n",
    "            bestScore = 1000\n",
    "            for key in boardState.keys():\n",
    "                if boardState[key] == ' ':\n",
    "                    boardState[key] = opponentLetter\n",
    "                    score = self.minimax(boardState, depth + 1, True, letter)\n",
    "                    boardState[key] = ' '\n",
    "                    if score < bestScore:\n",
    "                        bestScore = score\n",
    "        return bestScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimax with Alpha-Beta Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimaxAlphaBeta(Algorithm):\n",
    "    def __init__(self, boardGame):\n",
    "        super().__init__(boardGame)\n",
    "\n",
    "    def bestMove(self, boardGame, letter):\n",
    "        boardState = boardGame.getBoardState()\n",
    "        bestScore = -1000\n",
    "        bestMove = 0\n",
    "        for key in boardState.keys():\n",
    "            if boardState[key] == ' ':\n",
    "                boardState[key] = letter\n",
    "                score = self.minimax(boardState, 0, False, letter)\n",
    "                boardState[key] = ' '\n",
    "                if score > bestScore:\n",
    "                    bestScore = score\n",
    "                    bestMove = key\n",
    "        return bestMove\n",
    "\n",
    "    def minimax(self, boardState, depth, isMaximizing, letter, alpha=-1000, beta=1000, maxDepth=5):\n",
    "        \"\"\"Find the best score for the computer player.\n",
    "        Args: boardState (dict): The board state.\n",
    "        Args: depth (int): The depth of the tree.\n",
    "        Args: isMaximizing (bool): Whether the player is maximizing or not.\n",
    "        Args: letter (str): The letter of the computer player.\n",
    "        Args: alpha (int): The alpha value. \n",
    "        Args: beta (int): The beta value.\n",
    "        Args: maxDepth (int): The maximum depth of the tree.\n",
    "        Returns: int: The best score for the computer player.\"\"\"\n",
    "        gameLogic = TicTacToeGameLogic(self.boardGame)\n",
    "        opponentLetter = 'O' if letter == 'X' else 'X'\n",
    "        if(gameLogic.chkMarkForWin(letter)):\n",
    "            return 1\n",
    "        elif(gameLogic.chkMarkForWin(opponentLetter)):\n",
    "            return -1\n",
    "        elif(gameLogic.chkForDraw()):\n",
    "            return 0\n",
    "        elif depth >= maxDepth:\n",
    "            return 0\n",
    "\n",
    "        if isMaximizing:\n",
    "            bestScore = -1000\n",
    "            for key in boardState.keys():\n",
    "                if boardState[key] == ' ':\n",
    "                    boardState[key] = letter\n",
    "                    score = self.minimax(boardState, depth+1, False, letter, alpha, beta)\n",
    "                    boardState[key] = ' '\n",
    "                    bestScore = max(score, bestScore)\n",
    "                    alpha = max(alpha, score)\n",
    "                    if beta <= alpha:\n",
    "                        break\n",
    "        else:\n",
    "            bestScore = 1000\n",
    "            for key in boardState.keys():\n",
    "                if boardState[key] == ' ':\n",
    "                    boardState[key] = opponentLetter\n",
    "                    score = self.minimax(boardState, depth+1, True, letter, alpha, beta)\n",
    "                    boardState[key] = ' '\n",
    "                    bestScore = min(score, bestScore)\n",
    "                    beta = min(beta, score)\n",
    "                    if beta <= alpha:\n",
    "                        break\n",
    "        return bestScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Reinforcement Learning\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "\n",
    "class ValueIteration(Algorithm):\n",
    "    \"\"\" Value iteration algorithm for Tic Tac Toe\n",
    "    Args: boardGame (Board): The board game object\n",
    "    Initializes the value function, graph, and policy for the board game\"\"\"\n",
    "    \n",
    "    def __init__(self, boardGame):\n",
    "        super().__init__(boardGame)\n",
    "        self.policy =  {}\n",
    "        self.G = Graph()\n",
    "        self.value_function = {}\n",
    "        self.valueIteration()\n",
    "\n",
    "\n",
    "    def bestMove(self, boardGame, letter):\n",
    "        \"\"\"Find the best move for the computer player.\n",
    "        Args: boardGame (Board): The board game object\n",
    "        Args: letter (str): The letter of the computer player.\n",
    "        Returns: int: The position of the best move.\"\"\"\n",
    "\n",
    "        boardState = boardGame.getBoardState()\n",
    "        boardStateTuple = tuple(boardState.values())\n",
    "\n",
    "        if action := self.policy.get(boardStateTuple, None):\n",
    "            return self.extract_policy(action, boardStateTuple, boardState)\n",
    "        else:\n",
    "            print(\"No action found in policy for this state.\")\n",
    "\n",
    "\n",
    "    def extract_policy(self, action, boardStateTuple, boardState):\n",
    "        \"\"\" Extract the policy given a state\n",
    "        Args: action (int): The action to take\n",
    "        Args: boardStateTuple (tuple): The board state as a tuple\n",
    "        Args: boardState (dict): The board state as a dictionary\n",
    "        Returns: int: The position of the best move.\"\"\"\n",
    "\n",
    "        print(f\"Action found in policy: {action}\")\n",
    "\n",
    "        # Print the value of the current state\n",
    "        current_state_value = self.value_function.get(boardStateTuple, \"Unknown\")\n",
    "        print(f\"Value of current state: {current_state_value}\")\n",
    "\n",
    "        # If you can calculate the next state, you can print its value too\n",
    "        state_tuple = tuple(boardState.values())\n",
    "        neighbors = self.G.get_neighbors(state_tuple)\n",
    "        # print all the neighbors\n",
    "        for neighbor in neighbors:\n",
    "            print(f\"Neighbor: {neighbor}\")\n",
    "            print(f\"Value of neighbor: {self.value_function.get(neighbor, 'Unknown')}\")\n",
    "\n",
    "        # Print all possible actions and their values (if you have this information)\n",
    "        # print(f\"All possible actions and their values: {some_dictionary}\")\n",
    "\n",
    "        return action\n",
    "\n",
    "            \n",
    "    def valueIteration(self):\n",
    "        \"\"\" Perform value iteration to find the optimal policy and value function\"\"\"\n",
    "\n",
    "        game_logic = TicTacToeGameLogic(self.boardGame)\n",
    "        best_policy = {}\n",
    "        blankBoard = copy.deepcopy(self.boardGame)\n",
    "        graph = self.initialize(self.G, blankBoard, game_logic)\n",
    "        print(\"Graph initialized\")\n",
    "        self.value_function = {state: 0 for state in graph.nodes}\n",
    "        if self.converge(game_logic):\n",
    "            print(\"Converged\")\n",
    "        # print(f\"Value function: {self.value_function}\")\n",
    "\n",
    "\n",
    "    def initialize(self, graph, board, game_logic):\n",
    "        \"\"\" Initialize the graph with all possible states and actions\n",
    "        Args: graph (Graph): The graph object\n",
    "        Args: board (Board): The board object\n",
    "        Args: game_logic (GameLogic): The game logic object\n",
    "        Returns: Graph: The graph object with all possible states and actions\"\"\"\n",
    "        print(\"Initializing...\")\n",
    "        print(f\"Start state: {board.getBoardState()}\")\n",
    "        graph.add_node(tuple(board.getBoardState().values()))\n",
    "        queue = [(board, 'X')]  # Add the player to the queue\n",
    "        while queue:\n",
    "            state, current_player = queue.pop(0)\n",
    "            for action in board.getActions(state):\n",
    "                new_state = copy.deepcopy(state)\n",
    "                new_state.setCellState(action, current_player)\n",
    "                new_state_tuple = tuple(new_state.getBoardState().values())\n",
    "                \n",
    "                edge_reward = 0  # Default edge reward\n",
    "                \n",
    "                # Check if the new state is a terminal state\n",
    "                if game_logic.chkForWin(new_state):\n",
    "                    graph.add_node(new_state_tuple)\n",
    "                    if game_logic.chkMarkForWin('X', new_state):\n",
    "                        edge_reward = 1\n",
    "                        graph.nodes[new_state_tuple].value = 1\n",
    "                    elif game_logic.chkMarkForWin('O', new_state):\n",
    "                        edge_reward = -1\n",
    "                        graph.nodes[new_state_tuple].value = -1\n",
    "                    graph.add_edge(tuple(state.getBoardState().values()), new_state_tuple, edge_reward)\n",
    "                    continue  # Skip adding this state to the queue\n",
    "                elif game_logic.chkForDraw(new_state):\n",
    "                    graph.add_node(new_state_tuple)\n",
    "                    graph.nodes[new_state_tuple].value = 0\n",
    "                \n",
    "                if new_state_tuple not in graph.nodes:\n",
    "                    graph.add_node(new_state_tuple)\n",
    "                    next_player = 'O' if current_player == 'X' else 'X'\n",
    "                    queue.append((new_state, next_player))\n",
    "                \n",
    "                graph.add_edge(tuple(state.getBoardState().values()), new_state_tuple, edge_reward)\n",
    "                graph.update_edge_reward(tuple(state.getBoardState().values()), new_state_tuple)\n",
    "        \n",
    "        if not graph.nodes:\n",
    "            raise ValueError(\"Graph is empty\")\n",
    "        return graph\n",
    "\n",
    "\n",
    "    def converge(self, game_logic, gamma=0.9, epsilon=1e-4):\n",
    "        \"\"\" Converges the value function and policy\n",
    "        Args: game_logic (GameLogic): The game logic object\n",
    "        Args: gamma (float): The discount factor\n",
    "        Args: epsilon (float): The convergence factor (default 1e-4)\n",
    "        Returns: bool: True if the value function converged, False otherwise\"\"\"\n",
    "        print(\"Converging...\")\n",
    "        delta = float('inf')\n",
    "        i = 0\n",
    "        # Loop until the value function converges\n",
    "        while delta > epsilon:\n",
    "            delta = 0  # Reset delta for each iteration\n",
    "\n",
    "            # Iterate through all states\n",
    "            for curr_state in self.G.nodes.values():\n",
    "                old_value = self.value_function.get(curr_state.state, 0)  # Get the old value function for the state\n",
    "                best_action_value = self.calculateBestAction(curr_state.state, gamma)  # Calculate the best action value\n",
    "                # print(f\"Old value for state {curr_state.state}: {old_value}\")\n",
    "                # print(f\"New value for state {curr_state.state}: {best_action_value}\")\n",
    "\n",
    "                # Update the value function\n",
    "                self.value_function[curr_state.state] = best_action_value\n",
    "\n",
    "                # Update the policy\n",
    "                self.policy[curr_state.state] = self.calculateBestAction(curr_state.state, gamma, return_action=True)\n",
    "\n",
    "                # Calculate the change in the value function for this state\n",
    "                delta = max(delta, abs(old_value - best_action_value))\n",
    "                # Inside the converge method\n",
    "                # print(f\"Delta: {delta}\")  # To check the convergence value\n",
    "\n",
    "            i += 1\n",
    "        print(f\"Number of iterations: {i}\")\n",
    "\n",
    "        print(\"Value function converged.\")\n",
    "\n",
    "\n",
    "    def calculateBestAction(self, state_tuple, gamma, return_action=False):\n",
    "        \"\"\" Calculate the best action for a given state\n",
    "        Args: state_tuple (tuple): The state as a tuple\n",
    "        Args: gamma (float): The discount factor\n",
    "        Args: return_action (bool): Whether to return the action or the value\n",
    "        Returns: float: The value of the best action, or the best action itself\"\"\"\n",
    "\n",
    "        max_value = float('-inf')  # Initialize to negative infinity\n",
    "        best_action = None\n",
    "\n",
    "        # Get the neighbors (possible next states) for the current state\n",
    "        neighbors = self.G.get_neighbors(state_tuple)\n",
    "\n",
    "        # If there are no neighbors, return 0 or None based on the flag\n",
    "        if not neighbors:\n",
    "            self.value_function[state_tuple] = 0  # Update value function for terminal states\n",
    "            return 0 if not return_action else None\n",
    "\n",
    "        # Initialize a list to store the best actions\n",
    "        best_actions = []\n",
    "\n",
    "        # Iterate through each neighbor to find the best action(s)\n",
    "        for neighbor in neighbors:\n",
    "            reward = self.G.get_reward(state_tuple, neighbor)\n",
    "            value = reward + (gamma * self.value_function.get(neighbor, 0))  # Calculate the value of the action\n",
    "\n",
    "            if value > max_value:\n",
    "                max_value = value\n",
    "                best_actions = [self.new_X_position(state_tuple, neighbor) + 1]  # Reset the list with the new best action\n",
    "            elif value == max_value:\n",
    "                best_actions.append(self.new_X_position(state_tuple, neighbor) + 1)  # Add this action to the list of best actions\n",
    "\n",
    "        # Randomly choose one of the best actions\n",
    "        best_action = random.choice(best_actions)\n",
    "\n",
    "        # Update the value function for the current state\n",
    "        self.value_function[state_tuple] = max_value\n",
    "\n",
    "        return max_value if not return_action else best_action\n",
    "\n",
    "    \n",
    "    def new_X_position(self, state_tuple, chosen_neighbor):\n",
    "        \"\"\" Get the position of the new X in the chosen neighbor\n",
    "        Args: state_tuple (tuple): The state as a tuple\n",
    "        Args: chosen_neighbor (tuple): The chosen neighbor as a tuple\n",
    "        Returns: int: The position of the new X in the chosen neighbor\"\"\"\n",
    "        return [\n",
    "            i\n",
    "            for i, (current, next) in enumerate(zip(state_tuple, chosen_neighbor))\n",
    "            if current != next\n",
    "        ][0]\n",
    "    \n",
    "\n",
    "# def find_best_policy():\n",
    "#     best_policy = None\n",
    "#     best_value_function = None\n",
    "#     highest_average_reward = float('-inf')\n",
    "\n",
    "#     for _ in range(num_episodes):\n",
    "#         # Initialize value function and policy for this episode\n",
    "#         value_function = {}\n",
    "#         policy = {}\n",
    "\n",
    "#         # Run Value Iteration\n",
    "#         value_function, policy = run_value_iteration()\n",
    "\n",
    "#         # Evaluate the policy (you'll need to implement this)\n",
    "#         average_reward = evaluate_policy(policy)\n",
    "\n",
    "#         # Update the best policy if this one is better\n",
    "#         if average_reward > highest_average_reward:\n",
    "#             highest_average_reward = average_reward\n",
    "#             best_policy = policy\n",
    "#             best_value_function = value_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, state, value):\n",
    "        self.state = state\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"State: {self.state}, Value: {self.value}\"\n",
    "\n",
    "        \n",
    "\n",
    "class Graph:\n",
    "    def __init__(self):\n",
    "        self.nodes = {}\n",
    "        self.edges = {}\n",
    "\n",
    "    def add_node(self, state, value=0):\n",
    "        node = Node(state, value)\n",
    "        self.nodes[state] = node\n",
    "        return node\n",
    "\n",
    "    def add_edge(self, state1, state2, reward=0):\n",
    "        if state1 not in self.nodes:\n",
    "            self.add_node(state1)\n",
    "        if state2 not in self.nodes:\n",
    "            self.add_node(state2)\n",
    "        if state1 not in self.edges:\n",
    "            self.edges[state1] = {}\n",
    "        \n",
    "        # Use the value of the destination node as the reward\n",
    "        reward = self.nodes[state2].value if self.nodes[state2].value != 0 else reward\n",
    "        # print(f\"Reward: {reward}\")\n",
    "        self.edges[state1][state2] = reward\n",
    "\n",
    "    def update_edge_reward(self, state1, state2):\n",
    "        if state1 in self.edges and state2 in self.edges[state1]:\n",
    "            self.edges[state1][state2] = self.nodes[state2].value\n",
    "            # print(f\"Updated edge reward: {self.edges[state1][state2]}\")\n",
    "\n",
    "\n",
    "\n",
    "    def get_neighbors(self, state):\n",
    "        return [] if state not in self.edges else list(self.edges[state].keys())\n",
    "\n",
    "    def get_reward(self, state1, state2):\n",
    "        if state1 not in self.edges or state2 not in self.edges[state1]:\n",
    "            return 0\n",
    "        # print(self.edges[state1][state2])\n",
    "        return self.edges[state1][state2]\n",
    "\n",
    "    def print_graph(self):\n",
    "        for i, node in enumerate(self.nodes.values(), start=1):\n",
    "            print(f\"State {i} Node: {node}\")\n",
    "            neighbors = self.get_neighbors(node.state)\n",
    "            for neighbor in neighbors:\n",
    "                reward = self.get_reward(node.state, neighbor)\n",
    "                print(f\"  -> State {self.get_node_index(neighbor)} (Reward={reward})\")\n",
    "\n",
    "    # def get_node_index(self, state):\n",
    "    #     return list(self.nodes.keys()).index(state) + 1\n",
    "\n",
    "    # def count_winning_states(self):\n",
    "    #     return sum(1 for node in self.nodes.values() if node.value == 1)\n",
    "    \n",
    "    # def count_losing_states(self):\n",
    "    #     return sum(1 for node in self.nodes.values() if node.value == -1)\n",
    "\n",
    "    # def count_draw_states(self):\n",
    "    #     return sum(1 for node in self.nodes.values() if node.value == 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "class QLearning(Algorithm):\n",
    "    def __init__(self, boardGame, policy=None, explorationRate=0.2, learningRate=0.5, discountFactor=0.9):\n",
    "        super().__init__(boardGame)\n",
    "        self.boardGame = boardGame\n",
    "        self.game_logic = TicTacToeGameLogic(self.boardGame)\n",
    "        self.Qvalues = {}\n",
    "        self.explorationRate = explorationRate\n",
    "        self.learningRate = learningRate\n",
    "        self.discountFactor = discountFactor\n",
    "        if policy is None:\n",
    "            self.policy = {}\n",
    "            self.train(300)\n",
    "        else:\n",
    "            self.policy = policy\n",
    "\n",
    "\n",
    "\n",
    "    def export_policy(self) -> dict:\n",
    "        return self.policy\n",
    "\n",
    "\n",
    "    def bestMove(self, boardGame, letter):\n",
    "        \"\"\"Find the best move for the computer player.\n",
    "        Args: boardGame (Board): The board game object\n",
    "        Args: letter (str): The letter of the computer player.\n",
    "        Returns: int: The position of the best move.\"\"\"\n",
    "\n",
    "        boardState = boardGame.getBoardState()\n",
    "        print(f\"Board state: {boardState}\")\n",
    "        boardStateTuple = tuple(boardState.values())\n",
    "\n",
    "        if action := self.policy.get(boardStateTuple, None):\n",
    "            return self.extract_policy(action, boardStateTuple, boardState)\n",
    "        print(\"No action found in policy for this state.\")\n",
    "        # Fallback policy: Choose a random available action\n",
    "        possible_actions = [\n",
    "            i + 1\n",
    "            for i in range(boardGame.getBoardDimensions() ** 2)\n",
    "            if boardGame.getCellState(i + 1) == ' '\n",
    "        ]\n",
    "        return random.choice(possible_actions)  # or some heuristic\n",
    "\n",
    "\n",
    "\n",
    "    def extract_policy(self, action, boardStateTuple, boardState):\n",
    "        \"\"\"Extract the best move from the policy.\n",
    "        Args:\n",
    "            action (int): The action suggested by the policy.\n",
    "            boardStateTuple (tuple): The tuple representation of the board state.\n",
    "            boardState (dict): The dictionary representation of the board state.\n",
    "        Returns:\n",
    "            int: The position of the best move.\n",
    "        \"\"\"\n",
    "        if boardState[action] == ' ':\n",
    "            return action\n",
    "    \n",
    "\n",
    "    def qLearning(self):\n",
    "    # Entry point for the Q learning algorithm\n",
    "        current_state = self.boardGame\n",
    "        player_queue = ['X']\n",
    "        while not self.game_logic.chkForWin(current_state) and not self.game_logic.chkForDraw(current_state):\n",
    "            action = self.chooseAction(current_state, self.explorationRate)\n",
    "            # current_state.printBoard()\n",
    "            # print(f\"Action: {action}\")\n",
    "            current_player = player_queue.pop(0)\n",
    "            # print(f\"Current player: {current_player}\")\n",
    "            new_state, next_player = self.takeAction(current_state, action, current_player)\n",
    "            reward = self.observeReward(new_state)  # Call observeReward here\n",
    "            # print(f\"new state: {new_state.getBoardState()}\")\n",
    "            self.updateQValues(current_state, action, reward, new_state)  # Assume you'll implement this method to update Q-values\n",
    "            player_queue.append(next_player)\n",
    "            current_state = new_state\n",
    "            # current_state.printBoard()\n",
    "        # print(\"Game over\")\n",
    "        # for values in self.Qvalues:\n",
    "        #     print(f\"State: {values[0].getBoardState()}, Action: {values[1]}, Q value: {self.Qvalues[values]}\")\n",
    "\n",
    "\n",
    "\n",
    "    def chooseAction(self, current_state, explorationRate):\n",
    "        \"\"\"Choose an action based on the current state and exploration rate\n",
    "        Args: current_state (Board): The current state of the board\n",
    "        Args: explorationRate (float): The exploration rate\n",
    "        Returns: int: The action to take\"\"\"\n",
    "\n",
    "        possible_actions = [\n",
    "            i + 1\n",
    "            for i in range(current_state.getBoardDimensions() ** 2)\n",
    "            if current_state.getCellState(i + 1) == ' '\n",
    "        ]\n",
    "        # print(f\"Possible actions: {possible_actions}\")\n",
    "        random_value = random.uniform(0, 1)\n",
    "        if random_value < explorationRate:\n",
    "            return random.choice(possible_actions)\n",
    "        # negative infinity\n",
    "        max_q_value = float('-inf')\n",
    "        best_action = []\n",
    "        for action in possible_actions:\n",
    "            # print(f\"Action: {action}\")\n",
    "            # if current state, action is not in q values, set it to 0\n",
    "            if (current_state, action) not in self.Qvalues:\n",
    "                # print(f\"State: {current_state.getBoardState()}, Action: {action} not in Q values\")\n",
    "                self.Qvalues[(current_state, action)] = 0\n",
    "                # print(f\"Q value: {self.Qvalues[(current_state, action)]}\")\n",
    "                # get q value for current state, action\n",
    "            q_value = self.Qvalues[(current_state, action)]\n",
    "            if q_value > max_q_value:\n",
    "                max_q_value = q_value\n",
    "                best_action = [action]\n",
    "            elif q_value == max_q_value:\n",
    "                best_action.append(action)\n",
    "        return random.choice(best_action)\n",
    "\n",
    "\n",
    "    def takeAction(self, current_state, action, current_player):\n",
    "\n",
    "        new_state = copy.deepcopy(current_state)\n",
    "        new_state.setCellState(action, current_player)\n",
    "        # swap player\n",
    "        next_player = 'O' if current_player == 'X' else 'X'\n",
    "        return new_state, next_player\n",
    "\n",
    "\n",
    "\n",
    "    def observeReward(self, new_state):\n",
    "        LOSE_REWARD = -1\n",
    "        DRAW_REWARD = 0\n",
    "\n",
    "        if self.game_logic.chkMarkForWin('X', new_state):\n",
    "            # print(\"X wins\")\n",
    "            return 1\n",
    "        elif self.game_logic.chkMarkForWin('O', new_state):\n",
    "            # print(\"O wins\")\n",
    "            return LOSE_REWARD\n",
    "        elif self.game_logic.chkForDraw(new_state):\n",
    "            # print(\"Draw\")\n",
    "            return DRAW_REWARD\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "    def updateQValues(self, current_state, action, reward, new_state):\n",
    "    # Step 1: Get current Q-value\n",
    "        current_q_value = self.Qvalues.get((current_state, action), 0)\n",
    "        # print(f\"Current Q value: {current_q_value}\")\n",
    "        # print(f\"current state: {current_state.getBoardState()}\")\n",
    "\n",
    "        # Step 2: Get max Q-value for new state\n",
    "        possible_actions_new_state = [\n",
    "            i + 1\n",
    "            for i in range(current_state.getBoardDimensions() ** 2)\n",
    "            if current_state.getCellState(i + 1) == ' '\n",
    "        ]\n",
    "\n",
    "        max_new_state_value = max(\n",
    "            self.Qvalues.get((new_state, a), 0) for a in possible_actions_new_state\n",
    "        )\n",
    "\n",
    "        # If new_state is terminal, set max_new_state_value to 0\n",
    "        if self.game_logic.chkForWin(new_state) or self.game_logic.chkForDraw(new_state):\n",
    "            max_new_state_value = 0\n",
    "\n",
    "        # Step 3: Update Q-value using Q-Learning formula\n",
    "        updated_q_value = (1 - self.learningRate) * current_q_value + self.learningRate * (reward + self.discountFactor * max_new_state_value)\n",
    "\n",
    "        # Update the Q-value in the dictionary\n",
    "        self.Qvalues[(current_state, action)] = updated_q_value\n",
    "        # print(f\"Updated Q value: {updated_q_value}\")\n",
    "\n",
    "\n",
    "\n",
    "    def updatePolicy(self):\n",
    "        for state, _ in self.Qvalues.keys():\n",
    "            # Convert the TicTacToeBoard object to a tuple\n",
    "            state_tuple = tuple(state.getBoardState().values())\n",
    "            \n",
    "            possible_actions = [\n",
    "                i + 1\n",
    "                for i in range(state.getBoardDimensions() ** 2)\n",
    "                if state.getCellState(i + 1) == ' '\n",
    "            ]\n",
    "            \n",
    "            # Update the policy using the tuple representation of the state\n",
    "            self.policy[state_tuple] = max(\n",
    "                (\n",
    "                    (action, self.Qvalues.get((state, action), 0))\n",
    "                    for action in possible_actions\n",
    "                ),\n",
    "                key=lambda x: x[1],\n",
    "            )[0]\n",
    "            \n",
    "        # print(\"Policy updated\")\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        for _ in range(num_episodes):\n",
    "            self.qLearning()\n",
    "            self.updatePolicy()\n",
    "        print(\"Training complete\")\n",
    "\n",
    "\n",
    "    def checkConvergence(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "\n",
    "    def load_model(self, dimensions):\n",
    "        if dimensions not in self.models:\n",
    "            try:\n",
    "                with open(f'qlearning_{dimensions}x{dimensions}.pkl', 'rb') as f:\n",
    "                    self.models[dimensions] = pickle.load(f)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"No pre-trained model found for {dimensions}x{dimensions}. Training now...\")\n",
    "                self.train_and_save_model(dimensions)\n",
    "        return self.models[dimensions]\n",
    "\n",
    "    def train_and_save_model(self, dimensions):\n",
    "        game_factory = TicTacToeCreator(dimensions)\n",
    "        initial_board = game_factory.board  # Assuming this gives you the initial board\n",
    "        q_learning_instance = QLearning(initial_board, policy=None, explorationRate=0.2, learningRate=0.5, discountFactor=0.9)\n",
    "        q_learning_policy = q_learning_instance.export_policy()\n",
    "        \n",
    "        with open(f'qlearning_{dimensions}x{dimensions}.pkl', 'wb') as f:\n",
    "            pickle.dump(q_learning_policy, f)\n",
    "        \n",
    "        self.models[dimensions] = q_learning_policy\n",
    "        print(f\"Trained and saved Q-Learning policy for {dimensions}x{dimensions} board.\")\n",
    "\n",
    "# Initialize the ModelManager\n",
    "model_manager = ModelManager()\n",
    "\n",
    "# Train and save models for 3x3, 4x4, and 5x5 boards\n",
    "for dimensions in [3, 4, 5]:\n",
    "    # check if model exists\n",
    "    if model_manager.load_model(dimensions):\n",
    "        continue\n",
    "    model_manager.train_and_save_model(dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Tree Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "\n",
    "class TreeNode:\n",
    "    \"\"\"\n",
    "    A node in the MCTS tree.\n",
    "    Each node keeps track of its own value Q, prior probability P, and\n",
    "    its visit-count-adjusted prior score u.\n",
    "    \"\"\"\n",
    "    def __init__(self, state: Any, parent: 'TreeNode', visit_count: int, value: float, action: Any = None):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.visit_count = visit_count\n",
    "        self.value = value\n",
    "        self.action = action\n",
    "        self.children: List[TreeNode] = []\n",
    "        # print(f\"TreeNode initialized with state type: {type(self.state)}, content: {self.state.getBoardState() if self.state else None}\")\n",
    "\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"State: {self.state}, Visit Count: {self.visit_count}, Value: {self.value}\"\n",
    "\n",
    "class Tree:\n",
    "    \"\"\"\n",
    "    A tree in the MCTS algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.root: TreeNode = None\n",
    "\n",
    "    def set_root(self, state: Any):\n",
    "        self.root = TreeNode(state, None, 0, 0)\n",
    "\n",
    "    def add_child(self, parent: TreeNode, state: Any, action: Any) -> TreeNode:\n",
    "        node = TreeNode(state, parent, 0, 0, action)\n",
    "        parent.children.append(node)\n",
    "        return node\n",
    "\n",
    "    def get_action(self, node: TreeNode) -> Any:\n",
    "        return node.action\n",
    "\n",
    "    def get_node(self, state: Any) -> TreeNode:\n",
    "        return self._get_node(state, self.root)\n",
    "\n",
    "    def get_state(self, node: TreeNode) -> Any:\n",
    "        # print(f\"Retrieved state type: {type(node.state)}, content: {node.state}\")\n",
    "        return node.state\n",
    "    \n",
    "    def get_root(self) -> TreeNode:\n",
    "        return self.root\n",
    "\n",
    "    def get_children(self, node: TreeNode) -> List[TreeNode]:\n",
    "        # print(f\"Retrieved children type: {type(node.children)}, content: {node.children}\")\n",
    "        return node.children\n",
    "    \n",
    "    def get_parent(self, node: TreeNode) -> TreeNode:\n",
    "        return node.parent\n",
    "\n",
    "    def get_visit_count(self, node: TreeNode) -> int:\n",
    "        return node.visit_count\n",
    "\n",
    "    def get_value(self, node: TreeNode) -> float:\n",
    "        return node.value\n",
    "\n",
    "    def increment_visit_count(self, node: TreeNode):\n",
    "        node.visit_count += 1\n",
    "        \n",
    "    def add_value(self, node: TreeNode, value: float):\n",
    "        node.value = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from collections import deque\n",
    "\n",
    "class MonteCarloTreeSearch(Algorithm):\n",
    "    \"\"\"\n",
    "    Monte Carlo Tree Search algorithm for Tic Tac Toe\n",
    "    \"\"\"\n",
    "    def __init__(self, board_game: Any, q_learning_policy: dict, simulation_limit: int = 100, exploration_constant: float = 1.414):\n",
    "        super().__init__(board_game)\n",
    "        self.tree = Tree()\n",
    "        self.tree.set_root(board_game)\n",
    "        self.game_logic = TicTacToeGameLogic(board_game)\n",
    "        self.policy = q_learning_policy  # Importing the Q-Learning policy\n",
    "        self.q_values = {}\n",
    "        self.simulation_count = 0\n",
    "        self.simulation_limit = simulation_limit\n",
    "        self.exploration_constant = exploration_constant\n",
    "        self.player_queue = deque(['X', 'O'])\n",
    "\n",
    "    def bestMove(self, boardGame: Board, letter: str) -> int:\n",
    "        \"\"\"Find the best move for the computer player.\n",
    "        Args: boardGame (Board): The board game object\n",
    "        Args: letter (str): The letter of the computer player.\n",
    "        Returns: int: The position of the best move.\n",
    "        \"\"\"\n",
    "\n",
    "        boardState = boardGame.getBoardState()\n",
    "        boardStateTuple = tuple(boardState.values())\n",
    "\n",
    "        # Check if the current state exists in the policy\n",
    "        if action := self.policy.get(boardStateTuple, None):\n",
    "            return self.extract_policy(action, boardStateTuple, boardState)\n",
    "\n",
    "        # Fallback policy: Choose a random available action\n",
    "        possible_actions = [\n",
    "            i + 1\n",
    "            for i in range(boardGame.getBoardDimensions() ** 2)\n",
    "            if boardGame.getCellState(i + 1) == ' '\n",
    "        ]\n",
    "        return random.choice(possible_actions)  # or some other heuristic\n",
    "\n",
    "        \n",
    "\n",
    "    def extract_policy(self, action, boardStateTuple, boardState):\n",
    "        \"\"\"Extract the best move from the policy.\n",
    "        Args:\n",
    "            action (int): The action suggested by the policy.\n",
    "            boardStateTuple (tuple): The tuple representation of the board state.\n",
    "            boardState (dict): The dictionary representation of the board state.\n",
    "        Returns:\n",
    "            int: The position of the best move.\n",
    "        \"\"\"\n",
    "        if boardState[action] == ' ':\n",
    "            return action\n",
    "\n",
    "\n",
    "    def import_ql_policy(self, q_learning_policy: dict):\n",
    "        \"\"\" \n",
    "        Import the Q-Learning policy\n",
    "        \"\"\"\n",
    "        self.policy = q_learning_policy\n",
    "\n",
    "    def select(self):\n",
    "        current_node = self.tree.get_root()  # Start at the root node\n",
    "        while not self.is_leaf_node(current_node):\n",
    "            children = self.tree.get_children(current_node)\n",
    "            # Use UCT to select the best child node to explore\n",
    "            best_child = max(children, key=lambda child: self.uct_value(current_node, child))\n",
    "            current_node = best_child\n",
    "        return current_node  # Return the leaf node for expansion\n",
    "\n",
    "    def uct_value(self, parent, child):\n",
    "        w = self.tree.get_value(child)  # Total reward of the node\n",
    "        n = self.tree.get_visit_count(child)  # Number of times the node has been visited\n",
    "        N = self.tree.get_visit_count(parent)  # Number of times the parent has been visited\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if n == 0:\n",
    "            return float('inf')\n",
    "        \n",
    "        # print type of child\n",
    "        # print(f\"Child type: {type(child)}\")\n",
    "        # print(f\"w: {w}, n: {n}, N: {N}\")\n",
    "        return w / n + self.exploration_constant * ((N / n) ** 0.5)\n",
    "\n",
    "    def is_leaf_node(self, node):\n",
    "        return len(self.tree.get_children(node)) == 0\n",
    "\n",
    "\n",
    "\n",
    "    def expand(self, node):\n",
    "        \"\"\"\n",
    "        Expand the tree by adding new nodes based on possible actions.\n",
    "        \"\"\"\n",
    "        # print(\"Expanding...\")\n",
    "        current_state = self.tree.get_state(node)  # Assuming get_state returns a TicTacToeBoard object\n",
    "        board_dimension = current_state.getBoardDimensions()\n",
    "        \n",
    "        possible_actions = [\n",
    "            i + 1\n",
    "            for i in range(board_dimension ** 2)\n",
    "            if current_state.getCellState(i + 1) == ' '\n",
    "        ]\n",
    "        \n",
    "        for action in possible_actions:\n",
    "            new_state = copy.deepcopy(current_state)\n",
    "            \n",
    "            # Debugging setCellState\n",
    "            returned_state = new_state.setCellState(action, \"X\") \n",
    "            # Add the new state as a child node\n",
    "            new_node = self.tree.add_child(node, new_state, action)\n",
    "            \n",
    "\n",
    "\n",
    "    def is_terminal_state(self, state):\n",
    "        return self.game_logic.chkForDraw(state) or self.game_logic.chkForWin(state)\n",
    "\n",
    "\n",
    "    def simulate(self, node):\n",
    "        \"\"\"\n",
    "        Use Q-Learning policy to simulate the game from the given node to a terminal state.\n",
    "        \"\"\"\n",
    "        # print(\"Simulating...\")\n",
    "        current_state = self.tree.get_state(node)\n",
    "        local_player_queue = deque(['O', 'X'])\n",
    "        # Ensure that the node's state is a TicTacToeBoard object\n",
    "        if not isinstance(current_state, TicTacToeBoard):\n",
    "            raise ValueError(\"Node's state must be a TicTacToeBoard object.\")\n",
    "            \n",
    "        while not self.is_terminal_state(current_state):\n",
    "            # Use Q-Learning policy to choose an action\n",
    "            current_player = local_player_queue.popleft()\n",
    "            state_str = str(current_state.getBoardState())  # Convert the board state to a string or some hashable form\n",
    "            if state_str in self.policy:\n",
    "                action = max(self.policy[state_str], key=self.policy[state_str].get)  # Choose the action with the highest Q-value\n",
    "            else:\n",
    "                # If the state is not in the policy, choose a random action\n",
    "                possible_actions = [\n",
    "                    i + 1\n",
    "                    for i in range(current_state.getBoardDimensions() ** 2)\n",
    "                    if current_state.getCellState(i + 1) == ' '\n",
    "                ]\n",
    "                action = random.choice(possible_actions)\n",
    "            \n",
    "            new_state = copy.deepcopy(current_state)\n",
    "            new_state.setCellState(action, current_player)\n",
    "            local_player_queue.append(current_player)\n",
    "            current_state = new_state\n",
    "\n",
    "        # At the end of the while loop, you have a terminal state in `current_state`\n",
    "        reward = 0  # Initialize reward\n",
    "        if self.game_logic.chkMarkForWin('X', current_state) and self.game_logic.chkForWin(current_state):\n",
    "            # print(\"X wins\")\n",
    "            reward = 1\n",
    "        elif self.game_logic.chkMarkForWin('O', current_state) and self.game_logic.chkForWin(current_state):\n",
    "            # print(\"O wins\")\n",
    "            reward = -1\n",
    "        elif self.game_logic.chkForDraw(current_state):\n",
    "            # print(\"Draw\")\n",
    "            reward = 0\n",
    "        # print(f\"Reward: {reward}\")\n",
    "        return reward\n",
    "\n",
    "\n",
    "\n",
    "    def backpropagate(self, leaf_node, reward):\n",
    "        current_node = leaf_node  # Start at the leaf node\n",
    "        while current_node is not None:  # Traverse back to the root\n",
    "            self.tree.increment_visit_count(current_node)  # Increment the visit_count of the current node\n",
    "            self.tree.add_value(current_node, reward)  # Update the value of the current node based on the reward\n",
    "\n",
    "            # Check if the current node is the root\n",
    "            if current_node == self.tree.get_root():\n",
    "                break  # Stop backpropagation if you've reached the root\n",
    "\n",
    "            current_node = self.tree.get_parent(current_node)  # Move to the parent node\n",
    "            # print(f\"Current node in backpropagate: Type - {type(current_node)}, Content - {current_node.state.getBoardState() if current_node else None}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Optional, Deque\n",
    "from collections import deque\n",
    "\n",
    "class Integration:\n",
    "    def __init__(self, \n",
    "                 mcts_instance: MonteCarloTreeSearch, \n",
    "                 q_learning_instance: QLearning, \n",
    "                 board_game: Any, \n",
    "                 simulation_limit: int = 10, \n",
    "                 exploration_constant: float = 1.414):\n",
    "        \"\"\"\n",
    "        Initialize the integration class with instances of MonteCarloTreeSearch and QLearning.\n",
    "        \n",
    "        Parameters:\n",
    "            mcts_instance: An instance of MonteCarloTreeSearch.\n",
    "            q_learning_instance: An instance of QLearning.\n",
    "            board_game: The initial board state.\n",
    "            simulation_limit: The limit for MCTS simulations.\n",
    "            exploration_constant: The exploration constant for MCTS.\n",
    "        \"\"\"\n",
    "        self.mcts = mcts_instance\n",
    "        self.q_learning = q_learning_instance\n",
    "        self.board_game = board_game\n",
    "        self.simulation_limit = simulation_limit\n",
    "        self.exploration_constant = exploration_constant\n",
    "        self.player_queue = deque(['X', 'O'])\n",
    "\n",
    "    def initialize(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the mcts tree with the root as the current board state.\n",
    "        \"\"\"\n",
    "        self.mcts.tree.set_root(self.board_game)\n",
    "        print(\"Initialized\")\n",
    "\n",
    "    def action_selection(self) -> Any:\n",
    "        \"\"\"\n",
    "        Use Q-Learning policy to select an action, but use MCTS for states where q-learning is uncertain.\n",
    "        \n",
    "        Returns:\n",
    "            The selected action.\n",
    "        \"\"\"\n",
    "        # Step 1: Retrieve Current State\n",
    "        current_state = self.board_game.getBoardState()\n",
    "\n",
    "        # Step 2: Q-Learning Action\n",
    "        q_learning_action = self.q_learning.bestMove(self.board_game, self.player_queue[0])  # Assuming 'X' is the current player\n",
    "\n",
    "        if is_uncertain := self.check_uncertainty(\n",
    "            current_state, q_learning_action\n",
    "        ):\n",
    "            mcts_action = self.mcts_action(current_state)\n",
    "            return mcts_action\n",
    "\n",
    "        # Step 5: Return Action\n",
    "        return q_learning_action\n",
    "\n",
    "    def check_uncertainty(self, current_state, q_learning_action) -> bool:\n",
    "        \"\"\"\n",
    "        Check if Q-Learning is uncertain about the chosen action.\n",
    "        \n",
    "        Args:\n",
    "            current_state: The current board state.\n",
    "            q_learning_action: The action chosen by Q-Learning.\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if uncertain, False otherwise.\n",
    "        \"\"\"\n",
    "        # Step 1: Retrieve Q-Values\n",
    "        possible_actions = [\n",
    "            i + 1\n",
    "            for i in range(self.board_game.getBoardDimensions() ** 2)\n",
    "            if self.board_game.getCellState(i + 1) == ' '\n",
    "        ]\n",
    "        q_values = [self.q_learning.Qvalues.get((current_state, action), 0) for action in possible_actions]\n",
    "        \n",
    "        # Step 2: Check for Multiple Maxima\n",
    "        max_q_value = max(q_values)\n",
    "        count_max_q_value = q_values.count(max_q_value)\n",
    "        \n",
    "        # Step 3: Return Uncertainty Status\n",
    "        return count_max_q_value > 1\n",
    "\n",
    "\n",
    "    def mcts_action(self, current_state) -> Any:\n",
    "        # Step 1: Initialize the Tree's Root\n",
    "        self.mcts.tree.set_root(current_state)\n",
    "        \n",
    "        # Step 2: Run Simulations\n",
    "        for _ in range(self.simulation_limit):\n",
    "            leaf_node = self.mcts.select()\n",
    "            self.mcts.expand(leaf_node)\n",
    "            reward = self.mcts.simulate(leaf_node)\n",
    "            self.mcts.backpropagate(leaf_node, reward)\n",
    "\n",
    "      \n",
    "        # Step 3: Choose Best Action\n",
    "        root = self.mcts.tree.get_root()\n",
    "        children = self.mcts.tree.get_children(root)\n",
    "        \n",
    "        if not children:\n",
    "            raise ValueError(\"No children found for the root node.\")\n",
    "        \n",
    "        # Handle zero-division error\n",
    "        best_child = max(children, key=lambda child: self.mcts.tree.get_value(child) / self.mcts.tree.get_visit_count(child) if self.mcts.tree.get_visit_count(child) != 0 else 0)\n",
    "        \n",
    "        return self.mcts.tree.get_action(best_child)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def tree_update(self, action_taken) -> None:\n",
    "        \"\"\"\n",
    "        Update the tree based on the latest action and state.\n",
    "        \"\"\"\n",
    "        current_root = self.mcts.tree.get_root()\n",
    "\n",
    "        # Find the new root based on the action taken\n",
    "        children = self.mcts.tree.get_children(current_root)\n",
    "        new_root = next(\n",
    "            (\n",
    "                child\n",
    "                for child in children\n",
    "                if self.mcts.tree.get_action(child) == action_taken\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "        if new_root is None:\n",
    "            raise ValueError(\"Action taken does not correspond to any child of the current root.\")\n",
    "\n",
    "        # Prune the tree by setting the new root\n",
    "        # print(f\"New root type: {type(new_root.state.getBoardState())}\")\n",
    "        self.mcts.tree.set_root(new_root)\n",
    "        return new_root\n",
    "        \n",
    "\n",
    "\n",
    "    def policy_update(self) -> None:\n",
    "        \"\"\"\n",
    "        Update the policy based on the latest action and state.\n",
    "        \"\"\"\n",
    "        # TODO: Implement policy update logic\n",
    "        pass\n",
    "\n",
    "    def efficiency_guide(self) -> Optional[List[Any]]:\n",
    "        \"\"\"\n",
    "        Provide guidance or metrics for improving the efficiency of the algorithms.\n",
    "        \n",
    "        Returns:\n",
    "            A list of suggestions or metrics, or None.\n",
    "        \"\"\"\n",
    "        # TODO: Implement efficiency guide logic\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learning_policy = model_manager.load_model(5)\n",
    "# print(f\"Q-Learning policy for 3x3 board: {q_learning_policy}\")\n",
    "\n",
    "# with open('qlearning_3x3.pkl', 'rb') as f:\n",
    "#     data = pickle.load(f)\n",
    "# print(data)\n",
    "\n",
    "\n",
    "if q_learning_policy is not None:\n",
    "    # Initialize the TicTacToeCreator and QLearning instances\n",
    "    game_factory = TicTacToeCreator(5)\n",
    "    initial_board = game_factory.board  # Assuming this gives you the initial board\n",
    "    q_learning_instance = QLearning(boardGame=initial_board, policy=q_learning_policy, learningRate=0.5) # Assuming this sets the policy\n",
    "\n",
    "    # Initialize the MonteCarloTreeSearch instance\n",
    "    mcts_instance = MonteCarloTreeSearch(initial_board, q_learning_policy)\n",
    "\n",
    "    # Initialize the Integration instance\n",
    "    integration_instance = Integration(mcts_instance, q_learning_instance, initial_board)\n",
    "\n",
    "    # Initialize the Integration class\n",
    "    integration_instance.initialize()\n",
    "\n",
    "    # Test the mcts_action function\n",
    "    best_action = integration_instance.mcts_action(initial_board)\n",
    "    print(f\"Best action determined by MCTS: {best_action}\")\n",
    "\n",
    "    new_root = integration_instance.tree_update(best_action)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Could not load the model. Please train it first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def prompt_user(prompt, options):\n",
    "    \"\"\"Prompt the user to choose an option.\n",
    "    Args: prompt (str): The prompt to display to the user.\n",
    "    Args: options (list): The list of options to display to the user.\n",
    "    Returns: int: The option chosen by the user.\n",
    "    \"\"\"\n",
    "    print(prompt)\n",
    "    for i, option in enumerate(options):\n",
    "        print(f\"{i+1}. {option}\")\n",
    "    choice = int(input(\"Please choose an option: \"))\n",
    "    while choice < 1 or choice > len(options):\n",
    "        print(f\"Choice must be between 1 and {len(options)}!\")\n",
    "        choice = int(input(\"Please choose an option: \"))\n",
    "    return choice\n",
    "\n",
    "def choose_game():\n",
    "    \"\"\"Prompt the user to choose a game.\n",
    "    Returns: game_factory (AbstractGameFactory): The game factory for the game chosen by the user.\n",
    "    \"\"\"\n",
    "    prompt = \"Welcome to the Game Factory!\\nPlease choose a game:\"\n",
    "    options = [\"Tic Tac Toe\", \"Chess\", \"Backgammon\"]\n",
    "    choice = prompt_user(prompt, options)\n",
    "    if choice == 1:\n",
    "        dimensions = int(input(\"Please enter the board dimensions: \"))\n",
    "        return TicTacToeCreator(dimensions)\n",
    "    elif choice == 2:\n",
    "        return ChessCreator()\n",
    "    elif choice == 3:\n",
    "        return BackgammonCreator()\n",
    "\n",
    "def choose_algorithm():\n",
    "    \"\"\"Prompt the user to choose an algorithm.\n",
    "    Returns: int: The algorithm chosen by the user.\n",
    "    \"\"\"\n",
    "    prompt = \"Which Algorithm should player use?\"\n",
    "    options = [\"Minimax\", \"Minimax with Alpha Beta Pruning\", \"Value Iteration\", \"Q-Learning\", \"MCTS\", \"User Input\"]\n",
    "    return prompt_user(prompt, options)\n",
    "\n",
    "def create_player(game_factory, letter):\n",
    "    \"\"\"Create a player object\n",
    "    Args: game_factory (AbstractGameFactory): The game factory for the game chosen by the user.\n",
    "    Args: letter (str): the letter of the player. Must be 'X' or 'O'\n",
    "    Returns: player (Player): the player object. Must be a subclass of Player\n",
    "    \"\"\"\n",
    "    is_computer = input(f\"Is player {letter} a computer? (Y/N): \")\n",
    "    while is_computer not in ['Y', 'N']:\n",
    "        print(\"Invalid input!\")\n",
    "        is_computer = input(f\"Is player {letter} a computer? (Y/N): \")\n",
    "    algorithm_choice = choose_algorithm()\n",
    "    algorithm = game_factory.createAlgorithm(algorithm_choice)\n",
    "    return game_factory.createPlayer(letter, is_computer == 'Y', algorithm)\n",
    "\n",
    "def play_game():\n",
    "    \"\"\"Play the game.\"\"\"\n",
    "    game_factory = choose_game()\n",
    "    game_logic = game_factory.createGameLogic()\n",
    "    player_one = create_player(game_factory, 'X')\n",
    "    player_two = create_player(game_factory, 'O')\n",
    "    game_factory.board.printBoard()\n",
    "    start = time.time()\n",
    "    while not game_logic.chkForWin() and not game_logic.chkForDraw():\n",
    "        player_one.makeMove(game_factory.board)\n",
    "        game_factory.board.printBoard()\n",
    "        if game_logic.chkForWin():\n",
    "            print(\"Player\", player_one.letter, \"wins!\")\n",
    "            end = time.time()\n",
    "            print(\"Time taken: \", end - start)\n",
    "            break\n",
    "        elif game_logic.chkForDraw():\n",
    "            print(\"It's a draw!\")\n",
    "            end = time.time()\n",
    "            print(\"Time taken: \", end - start)\n",
    "            break\n",
    "        player_two.makeMove(game_factory.board)\n",
    "        game_factory.board.printBoard()\n",
    "        if game_logic.chkForWin():\n",
    "            print(\"Player\", player_two.letter, \"wins!\")\n",
    "            end = time.time()\n",
    "            print(\"Time taken: \", end - start)\n",
    "            break\n",
    "        elif game_logic.chkForDraw():\n",
    "            print(\"It's a draw!\")\n",
    "            end = time.time()\n",
    "            print(\"Time taken: \", end - start)\n",
    "            break\n",
    "\n",
    "play_game()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
